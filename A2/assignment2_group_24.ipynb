{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOSf9HeM1r5AAeph/0djx3x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acollin19/comp551/blob/Angele/A2/assignment2_group_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Process:\n",
        "\n",
        "### Task 1: Data Processing  \n",
        "\n",
        "\n",
        "1. IMDB Reviews\n",
        "2. 20 news groups: a multi-class labelled textual dataset\n",
        "\n",
        "\n",
        "### Task 2: Implement Logistic and Multiclass classifiers\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Multiclass Regression\n",
        "3. Comparisons using ROC curve\n",
        "\n",
        "\n",
        "### Task 3: Running Experiments\n",
        "\n",
        "\n",
        "\n",
        "1.    Most positive/negative Z-scores\n",
        "2.   Implement:  \n",
        "    * Binary classification on the IMDB Reviews\n",
        "\n",
        "  *   Multi-class classification on the 20 news group dataset\n",
        "\n",
        "3. Binary classification AUROC on IMDB data\n",
        "  *   Logistic Regression \n",
        "  *   KNN\n",
        "\n",
        "4. Multiclass classification accuracy\n",
        "\n",
        "5. Compare Accuracy of Models (Plot)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MPLXGJNqqK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "zk8gZ35HpnLe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Multiclass Regression and ROC curve comparisons\n",
        "from sklearn import model_selection\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression as sk_LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from nltk import word_tokenize, download\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!ls aclImdb\n",
        "!ls aclImdb/test\n",
        "!ls aclImdb/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHwlrYyNIEVK",
        "outputId": "00722c56-e19e-4d59-fdab-3e1641b0186f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  11.4M      0  0:00:07  0:00:07 --:--:-- 18.4M\n",
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n",
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB dataset preprocession\n",
        "IMDB_X_train, IMDB_y_train= load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "IMDB_X_test, IMDB_y_test = load_svmlight_file('aclImdb/test/labeledBow.feat') \n",
        "\n"
      ],
      "metadata": {
        "id": "RNMsl4tlfmJD"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st element in tuple represents the file its in, 2nd element represents the index of word it represents in vocab, float represents frequency\n",
        "# want to remove the words that appear in less than 1% of all documents # only appear in 250 documents\n",
        "# want to remove the words that appear in more than 50% of all documents # appear in 12500 documents\n",
        "sign = sum(IMDB_X_train.sign()) # 1 if the word appears in the text 0 if not\n",
        "good = ((sign < 12500) - (sign < 250))\n",
        "indices = sp.find(good)\n",
        "\n",
        "IMDB_X_train = IMDB_X_train[:,indices[1]]\n",
        "IMDB_X_test = IMDB_X_test[:,indices[1]]\n",
        "IMDB_X_train.todense()\n",
        "\n",
        "# sign = sum(IMDB_X_train.sign())\n",
        "# num_file_occurences_train = np.asarray(sign.sum(axis=0))# number of files each word appears in\n",
        "# # ndarray of the indices of the words excluding stop words and rare words\n",
        "# indices = np.intersect1d((np.flatnonzero(12500 > num_file_occurences_train)), (np.flatnonzero(250 < num_file_occurences_train)))\n",
        "\n",
        "# IMDB_X_train = IMDB_X_train[:,indices[1]]\n",
        "# IMDB_X_test = IMDB_X_test[:,indices[1]]\n",
        "\n",
        "print(indices[1])\n",
        "print(IMDB_X_train.shape[0])\n",
        "\n"
      ],
      "metadata": {
        "id": "CvVZpw-SLx6f",
        "outputId": "cbbc2951-6253-4d5a-adfe-578bf0091a72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  21   23   27 ... 1958 1967 1980]\n",
            "25000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py:291: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using < is inefficient, try using >= instead.\n",
            "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Z-Score Function"
      ],
      "metadata": {
        "id": "3k6XRFDaHUqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.extmath import sparse\n",
        "# Z - Score\n",
        "def zscore(X,y):\n",
        "\n",
        "  N = X.shape[0]\n",
        "  std = (IMDB_y_train- y.mean()) / y.std()\n",
        "  z = np. abs(sp.csr_matrix.dot(sp.csr_matrix.transpose(X), std))\n",
        "\n",
        "  return z\n",
        "\n",
        "df = pd.DataFrame(IMDB_X_train.toarray())\n",
        "df.insert(loc=0, column=\"A\", value=IMDB_y_train)\n",
        "IMDB_X_train = sp.csr_matrix(df.values)\n",
        "IMDB_X_train.todense()\n",
        "IMDB_arr = IMDB_X_train.toarray()\n",
        "\n",
        "zscores = zscore(IMDB_X_train,IMDB_y_train)\n",
        "\n",
        "highestzindex = np.argsort(zscores)[-200:][::-1] # top 200 highest z scores\n",
        "lowestzindex = np.argsort(zscores)[:200][::1] # worst 200 highest z scores\n",
        "zindex = np.concatenate((highestzindex, lowestzindex))\n",
        "\n",
        "# # get correct indexed feature set\n",
        "new_IMDB_X_train = IMDB_X_train[:,zindex].toarray()\n",
        "new_IMDB_X_test = IMDB_X_test[:,zindex].toarray()\n",
        "\n",
        "# # get correct indexed word list\n",
        "data = open('aclImdb/imdb.vocab')\n",
        "vocab = data.read().split('\\n')\n",
        "\n",
        "word_list = [vocab[i]for i in indices[1]]\n",
        "word_list = [word_list[i] for i in zindex]\n",
        "print((word_list))\n",
        "print(zscores)\n"
      ],
      "metadata": {
        "id": "jKj3Uv9UHUKI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6dc0bb-ebd5-4d36-cb7e-56a918e8c77f"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'some', 'other', '!', 'first', 'up', 'about', 'my', 'from', 'her', 'sure', 'just', 'even', 'life', 'his', 'or', 'best', 'there', 'has', 'like', 'into', 'its', 'little', 'never', 'much', 'new', 'did', 'story', 'which', 'out', 'while', 'after', 'really', 'an', 'could', 'wonderful', 'too', 'problem', 'them', 'because', 'last', 'father', 'full', 'bad', 'years', 'they', 'perhaps', 'very', 'episode', 'me', 'short', 'done', 'certainly', 'actors', 'someone', 'may', 'most', 'death', 'so', 'takes', 'quite', 'seems', 'girls', 'who', 'these', 'those', 'far', 'can', \"don't\", 'good', 'end', 'man', 'gets', 'before', 'anything', 'seen', 'made', 'turn', 'whole', 'come', 'without', 'she', 'still', 'name', 'get', 'great', 'children', 'what', 'should', 'type', 'when', 'fans', 'then', 'see', 'through', 'work', 'your', 'minutes', 'kill', 'people', 'know', 'real', 'audience', 'saw', 'show', 'if', 'let', 'their', '?', 'beginning', 'played', 'parts', 'here', \"i've\", 'two', 'take', 'big', 'women', 'set', 'american', 'different', 'known', 'able', 'ending', 'time', 'by', 'thing', 'place', 'fact', 'sense', 'title', 'screen', 'things', 'god', 'him', 'world', 'showing', 'our', 'boring', 'main', 'song', 'tv', 'also', 'funny', 'move', 'understand', 'scene', 'rent', 'anyone', 'many', 'actual', 'themselves', 'same', 'star', 'only', 'else', 'find', 'no', 'young', 'between', 'had', 'give', 'directors', 'well', 'need', 'production', 'john', 'does', 'both', 'person', 'later', 'than', 'everything', 'want', 'none', 'today', 'script', 'especially', 'himself', 'role', 'works', 'us', 'family', 'totally', 'scenes', 'sequence', 'gives', 'classic', 'must', 'mind', 'add', 'help', 'course', 'interested', 'goes', 'money', 'think', 'feeling', 'former', 'development', 'parents', 'culture', 'filled', 'attractive', 'unlike', 'noticed', 'build', 'steve', 'realized', 'treated', 'blame', 'game', 'emotional', 'students', 'difference', 'sleep', 'dangerous', 'highly', 'onto', 'image', 'superb', 'upon', 'cop', 'whose', 'neither', 'hilarious', 'leave', 'team', 'near', 'turning', 'collection', 'fairly', 'talented', 'carry', 'cast', 'genius', 'accept', 'listen', 'third', 'questions', 'rarely', 'themes', 'language', 'somewhat', 'wants', 'g', 'lame', 'points', 'describe', 'musical', 'south', 'worthy', 'accurate', 'talents', 'matter', 'remake', 'credits', 'bother', 'system', 'gorgeous', 'creative', 'date', 'local', 'introduced', 'general', 'key', 'wearing', 'pace', 'appear', 'brings', 'contains', 'past', 'personally', 'keeping', 'deeply', 'holes', 'telling', 'received', 'becoming', 'romance', 'falling', 'naturally', 'hand', 'professional', 'direct', 'warning', 'yeah', 'nearly', 'pass', 'information', 'adult', 'tone', 'finest', \"can't\", 'fake', 'compelling', 'start', 'popular', 'seeing', 'million', 'boyfriend', 'struggle', 'apartment', 'least', 'bill', 'male', 'award', 'led', 'super', 'hits', 'concerned', 'cover', 'average', 'avoid', 'technical', 'design', 'taking', 'purpose', 'put', 'creature', 'exist', 'include', 'mixed', 'kept', 'rare', 'rape', 'student', 'kid', 'pleasure', 'potential', 'society', 'empty', 'starting', 'movies', 'commentary', 'dies', 'edge', \"hasn't\", 'bar', 'intense', 'exciting', 'convincing', 'important', 'hardly', 'witty', 'sat', 'free', 'lets', 'discover', 'impact', 'includes', 'deal', 'reason', 'outstanding', 'present', 'everybody', 'happening', 'step', 'means', 'decided', 'mom', 'torture', 'usually', 'female', 'color', 'cause', 'high', 'criminal', 'perfect', 'changed', 'train', 'rated', 'grace', 'situations', 'america', 'seconds', 'say', 'record', 'kills', 'london', 'atmosphere', 'honestly', 'happens', 'equally', 'immediately', 'various', 'treat', 'basic', 'died', 'scott', 'detective', 'epic', 'painful', 'crew', 'brutal', 'intelligence', 'continue', 'pretty', 'fell', 'mystery']\n",
            "[8.66601970e+04 2.31270840e+03 4.81251525e+03 ... 3.55614585e+01\n",
            " 6.85840813e+00 6.90126402e+01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# QUESTION TO ANSWER: Do they make sense for calling a movie good and bad, respectively?\n",
        "\n",
        "posindexesZ = np.argsort(zindex)[-10:][::-1] #highest-lowest\n",
        "negindexesZ = np.argsort(zindex)[:10][::1] #lowest-highest\n",
        "\n",
        "pos = [word_list[i] for i in posindexesZ]\n",
        "neg = [word_list[i] for i in negindexesZ]\n",
        "\n",
        "print(pos)\n",
        "print(zscores[np.argsort(zscores)[-10:][::-1]])\n",
        "print(neg)\n",
        "print(zscores[np.argsort(zscores)[:10][::1]])\n",
        "\n",
        "# Answer : The top features with the most positive z-scores do partially make sense as it includes words like 'talents', 'naturally' which are generally used with a positive connotation. \n",
        "#          However, the top features with the most negative don't make sense in terms of calling a movie bad as it includes words such as 'who','he' which dont reveal any information as to if a movie is good or bad.\n"
      ],
      "metadata": {
        "id": "fGwIjJ5vSHOd",
        "outputId": "efdd8ad9-409c-4615-8549-ed92a7789d4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['received', 'discover', 'noticed', 'naturally', 'talents', 'concerned', 'empty', 'witty', 'hits', 'mixed']\n",
            "[86660.19703991  6723.9998858   5727.76839835  4812.51525205\n",
            "  3854.64022019  3589.98336753  3306.88369965  2937.47021926\n",
            "  2800.26686171  2558.4542105 ]\n",
            "['he', 'his', '!', 'by', 'an', 'who', 'they', 'from', 'so', 'like']\n",
            "[0.01938606 0.11033901 0.20391138 0.38087843 0.44247534 0.55575687\n",
            " 0.74857896 0.75157918 0.75452171 0.83523927]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading 20 news group dataset\n",
        "categories=['rec.sport.hockey','sci.electronics','talk.politics.guns','sci.space']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, remove=(['headers', 'footers', 'quotes']), shuffle=True, random_state=42)\n",
        "\n",
        "'''\n",
        "Cleaning: \n",
        "- filter out rare words, stopwords, and words that are not relevant to any of the 4 class labels.\n",
        "- MI to select the top M ∈ [10, 100] feature words per class and take the union of all top feature words to train your multiclass model.\n",
        "\n",
        "'''\n",
        "\n",
        "download('punkt') #tokenizer, run once\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "\n",
        "# Tokenize, remove stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    # Restricts string to alphabetic characters only\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "# text and ground truth labels\n",
        "texts, y = twenty_train.data, twenty_train.target\n",
        "\n",
        "corpus = [preprocess(text) for text in texts]\n"
      ],
      "metadata": {
        "id": "avXRl9Pq1YdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42e001e7-6284-484f-9c1b-4b178343937f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 : Data processing\n",
        "\n"
      ],
      "metadata": {
        "id": "sQE1MfRpbM2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : IMDB\n",
        "\n",
        " - X contain features based on the words in the movie reviews\n",
        " - y contain labels for whether the review sentiment is positive 1 or negative -1"
      ],
      "metadata": {
        "id": "kMcXOF5bbfbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB train df\n",
        "\n",
        "\n",
        "# QUESTION TO ANSWER: Do they make sense for calling a movie good and bad, respectively?\n"
      ],
      "metadata": {
        "id": "buYXwTcjbpWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : Twenty Train\n",
        "\n",
        "Text preprocessing, tokenizing and filtering of stopwords are all included in the CountVectorizer function\n"
      ],
      "metadata": {
        "id": "mpsPgK1HpYIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data) # builds a dict of features and transforms docs to feature vectors\n",
        "X_train_counts.shape\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ],
      "metadata": {
        "id": "aQpmgOaTorEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = twenty_train.data # Test content\n",
        "\n",
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data) # builds a dict of features and transforms docs to feature vectors\n",
        "X_train_counts.shape\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X = X_train_tfidf\n",
        "X.todense()\n",
        "twenty_train_arr = X.toarray()\n",
        "\n",
        "y_cat = twenty_train.target\n",
        "# convert array of class indices to one-hot encoded array\n",
        "y = np.zeros((y_cat.size, y_cat.max() + 1))\n",
        "y[np.arange(y_cat.size), y_cat] = 1\n",
        "\n",
        "#print(zscore(twenty_train_arr))"
      ],
      "metadata": {
        "id": "fNeX7J3j7XHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 : Implementing Classes"
      ],
      "metadata": {
        "id": "hBsr2kCsa4xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 : Logistic Regression Class"
      ],
      "metadata": {
        "id": "rUqXu5jLlL0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic function\n",
        "z = np.linspace(-5,5,100)\n",
        "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
        "\n",
        "# From class Logistic Regression collab\n",
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients \n",
        "        self.max_iters = max_iters                    #maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        N,D = x.shape\n",
        "        yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "        grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "        return grad\n",
        "        \n",
        "    def fit(self, x, y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x,np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf \n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x, y)\n",
        "            self.w = self.w - self.learning_rate * g \n",
        "            t += 1\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x,np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x,self.w))            #predict output\n",
        "        return yh\n",
        "\n",
        "# LogisticRegression.gradient = gradient  "
      ],
      "metadata": {
        "id": "yviSnmE3mQPo"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Logistic Regression on the IMDB data\n",
        "\n",
        "model = LogisticRegression(verbose=True, add_bias=False, max_iters=1e3,learning_rate=.3,epsilon=1e-4)\n",
        "fit = model.fit(new_IMDB_X_train,IMDB_y_train)\n",
        "\n",
        "y_train_pred = fit.predict(new_IMDB_X_train)\n",
        "y_test_pred = fit.predict(new_IMDB_X_test)\n",
        "\n",
        "# Threshold probabilities\n",
        "y_train_pred = (y_train_pred > 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "# Accuracy = correctly classified / total classified\n",
        "acc_train = sum(y_train_pred==IMDB_y_train)/len(IMDB_y_train)\n",
        "acc_test = sum(y_test_pred==IMDB_y_test)/len(IMDB_y_test)\n",
        "print(f\"train accuracy: {acc_train:.3f}; test accuracy: {acc_test:.3f}\")\n"
      ],
      "metadata": {
        "id": "LBTCwz2DcDwQ",
        "outputId": "cc295418-dac8-4487-a088-307a1976ee52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "terminated after 10000 iterations, with norm of the gradient equal to 42.51171188835378\n",
            "the weight found: [1.09631862e+05 5.84601651e+03 2.54261500e+03 1.77779362e+04\n",
            " 6.45713420e+03 5.20291477e+03 8.16262651e+03 5.56783582e+03\n",
            " 1.01976069e+04 9.04943293e+03 4.15936302e+02 8.63638783e+03\n",
            " 8.56664431e+03 4.41615813e+03 1.69949390e+04 1.08491904e+04\n",
            " 4.36275818e+03 9.71303881e+03 8.10682080e+03 9.99312091e+03\n",
            " 5.72429493e+03 3.73865065e+03 2.71719859e+03 2.74743866e+03\n",
            " 5.93357842e+03 1.60922569e+03 4.07019773e+03 5.70571136e+03\n",
            " 5.85463291e+03 9.67234060e+03 2.19411188e+03 3.45580595e+03\n",
            " 6.98479087e+03 1.26759746e+04 3.55756652e+03 2.83089954e+02\n",
            " 3.58408648e+03 2.02688724e+02 3.76468850e+03 4.31729446e+03\n",
            " 1.03117763e+03 1.64809241e+03 3.53889822e+02 5.46197519e+03\n",
            " 1.91858712e+03 1.19473277e+04 4.02730104e+02 8.05220448e+03\n",
            " 1.37076992e+03 5.31306469e+03 5.00890848e+02 1.15657805e+03\n",
            " 3.34448778e+02 2.84618687e+03 8.16614028e+02 2.26214042e+03\n",
            " 5.20049326e+03 5.84651310e+02 1.14279629e+04 1.58389303e+03\n",
            " 2.42078241e+03 2.37254188e+03 2.32207200e+02 1.19797292e+04\n",
            " 2.57919334e+03 2.14670849e+03 1.95733736e+03 5.66382773e+03\n",
            " 5.11457264e+03 8.05437440e+03 3.40491359e+03 2.67831433e+03\n",
            " 2.12497934e+03 1.93358589e+03 1.22377789e+03 3.95740001e+03\n",
            " 4.76428903e+03 1.08048792e+03 1.29961864e+03 2.07145901e+03\n",
            " 2.14765987e+03 7.98608508e+03 2.66535384e+03 1.20396938e+03\n",
            " 4.62029562e+03 4.49909426e+03 4.55409060e+02 8.56053951e+03\n",
            " 2.34819007e+03 2.01366066e+02 7.98260536e+03 1.09548848e+03\n",
            " 4.01632856e+03 6.44550827e+03 2.33510976e+03 1.95122544e+03\n",
            " 3.36351397e+03 1.26505766e+03 9.74647362e+02 4.66781562e+03\n",
            " 3.65919737e+03 2.15450751e+03 8.69053140e+02 2.00557897e+03\n",
            " 3.06411763e+03 9.02650415e+03 9.49333932e+02 6.00727040e+03\n",
            " 8.36469673e+03 4.63688418e+02 1.67401546e+03 9.27847146e+02\n",
            " 2.88219536e+03 2.07302006e+03 3.96748124e+03 1.60142090e+03\n",
            " 2.07950021e+03 1.24861093e+03 1.49545372e+03 1.37161234e+03\n",
            " 1.03081447e+03 2.54045772e+02 9.31687470e+02 9.97214016e+02\n",
            " 6.38791420e+03 1.29403473e+04 2.18042719e+03 1.53469430e+03\n",
            " 1.64222117e+03 9.87493842e+02 5.50088952e+02 1.48681379e+03\n",
            " 1.73666219e+03 8.57166798e+02 4.52969333e+03 2.27486267e+03\n",
            " 1.73164656e+02 1.60441520e+03 1.21585086e+03 1.45765357e+03\n",
            " 2.93405952e+02 1.62241544e+03 5.15645492e+03 2.52674555e+03\n",
            " 1.54804356e+02 6.51969894e+02 2.66211233e+03 1.38124134e+02\n",
            " 1.65481591e+03 3.82756018e+03 6.52204776e+02 8.62807128e+02\n",
            " 1.94642423e+03 1.24621144e+03 6.69187226e+03 8.75772234e+02\n",
            " 2.47106516e+03 6.92335492e+03 1.75190192e+03 1.59878021e+03\n",
            " 6.29766796e+03 2.02478025e+03 5.69524026e+02 5.08373909e+03\n",
            " 7.63330836e+02 7.27450428e+02 1.39285326e+03 3.09075680e+03\n",
            " 1.65038068e+03 1.04784946e+03 1.38445322e+03 5.50517923e+03\n",
            " 1.04485390e+03 2.20166236e+03 3.46566096e+02 8.67487476e+02\n",
            " 1.82497817e+03 1.55989522e+03 9.67573002e+02 1.50673899e+03\n",
            " 4.94407686e+02 2.22578269e+03 1.51249905e+03 5.08327836e+02\n",
            " 3.04563187e+03 2.76125220e+02 1.04016948e+03 1.16317087e+03\n",
            " 1.45873840e+03 8.11811166e+02 2.46124842e+02 8.27531334e+02\n",
            " 1.53577506e+03 1.60323876e+02 1.14025481e+03 1.07305405e+03\n",
            " 3.89128547e+03 4.32486846e+02 9.12030300e+01 1.63923840e+02\n",
            " 4.08364560e+02 2.50442796e+02 2.70363018e+02 1.88762106e+02\n",
            " 3.14523510e+02 1.43161596e+02 1.59361776e+02 2.56082856e+02\n",
            " 1.65721854e+02 1.46881644e+02 1.56001746e+02 6.73447524e+02\n",
            " 3.51003924e+02 1.92482154e+02 2.01602256e+02 1.69801890e+02\n",
            " 1.55281740e+02 6.09846816e+02 1.75681968e+02 1.98242220e+02\n",
            " 3.60484032e+02 4.52525046e+02 3.08283450e+02 5.31605928e+02\n",
            " 2.88363228e+02 5.20925808e+02 5.95086636e+02 4.27444782e+02\n",
            " 4.10284572e+02 1.79162010e+02 1.86242070e+02 3.16323522e+02\n",
            " 3.08283432e+02 1.73161944e+02 2.13314381e+03 2.41802688e+02\n",
            " 1.62361800e+02 1.76641986e+02 3.78124236e+02 2.59202880e+02\n",
            " 1.68961872e+02 2.25962538e+02 2.75403060e+02 5.17205760e+02\n",
            " 6.94687740e+02 1.72441908e+02 3.92764368e+02 4.36324854e+02\n",
            " 1.79641986e+02 5.30525904e+02 2.50082772e+02 1.66321878e+02\n",
            " 1.51801716e+02 1.42081608e+02 5.91246624e+02 3.09123474e+02\n",
            " 3.58803984e+02 2.10722376e+02 1.95122202e+02 2.00402214e+02\n",
            " 1.89122136e+02 2.21882502e+02 4.69445268e+02 1.65721878e+02\n",
            " 4.11964572e+02 1.88762136e+02 1.62961848e+02 2.92443294e+02\n",
            " 3.33603696e+02 3.41163780e+02 2.18162466e+02 6.75967578e+02\n",
            " 2.41802670e+02 1.44481644e+02 1.74721920e+02 1.86842118e+02\n",
            " 3.10203432e+02 1.35241542e+02 1.90442094e+02 3.75844164e+02\n",
            " 2.07122280e+02 1.39921596e+02 6.08646762e+02 1.73041896e+02\n",
            " 1.74121908e+02 1.70401866e+02 2.52122778e+02 4.31884860e+02\n",
            " 2.16362454e+02 1.76042004e+02 2.67843030e+02 2.61362958e+02\n",
            " 1.53001668e+02 1.90418122e+03 2.45642784e+02 2.03162310e+02\n",
            " 9.08050182e+02 2.86803246e+02 1.12693254e+03 1.91042178e+02\n",
            " 1.95242226e+02 1.69441938e+02 1.77482028e+02 1.68169873e+03\n",
            " 3.31683750e+02 3.45963816e+02 2.15402358e+02 1.70881956e+02\n",
            " 1.82161986e+02 1.49401620e+02 1.39681608e+02 2.69643060e+02\n",
            " 3.69004170e+02 4.13044662e+02 1.68361830e+02 1.75442010e+02\n",
            " 4.99445628e+02 2.29682616e+02 1.26457417e+03 1.71601968e+02\n",
            " 1.53841770e+02 1.97882262e+02 1.50361626e+02 4.06684488e+02\n",
            " 2.40122628e+02 1.83242100e+02 2.09042280e+02 6.28087068e+02\n",
            " 1.68841830e+02 3.33843672e+02 3.65044020e+02 1.40041620e+02\n",
            " 1.55521680e+02 4.08124563e+03 1.76041908e+02 2.13122322e+02\n",
            " 2.27162478e+02 1.94642232e+02 2.04602226e+02 1.79042058e+02\n",
            " 2.80923078e+02 2.92803210e+02 4.92725442e+02 3.25083690e+02\n",
            " 1.50481620e+02 1.49881734e+02 3.46083804e+02 1.82761980e+02\n",
            " 1.48921602e+02 2.05922238e+02 1.67521932e+02 3.90244296e+02\n",
            " 1.24429396e+03 2.19002508e+02 3.09243516e+02 2.25962460e+02\n",
            " 2.12042304e+02 1.71601980e+02 4.02604560e+02 3.84124224e+02\n",
            " 2.00402172e+02 1.53121776e+02 5.19485868e+02 5.07005592e+02\n",
            " 2.19002376e+02 2.79603192e+02 1.03177159e+03 1.68241950e+02\n",
            " 8.48049396e+02 2.65082886e+02 2.19962382e+02 2.16242340e+02\n",
            " 1.68481956e+02 2.67842916e+02 3.95284338e+02 1.79642082e+02\n",
            " 2.90283249e+03 1.57321680e+02 2.90043162e+02 2.50322718e+02\n",
            " 4.01404404e+02 2.37482730e+02 5.91126522e+02 2.22362562e+02\n",
            " 2.55362772e+02 3.31923624e+02 1.78801914e+02 2.86923120e+02\n",
            " 2.82123066e+02 3.05643498e+02 2.35202712e+02 1.58281854e+02\n",
            " 2.16842508e+02 3.10323378e+02 1.69681806e+02 1.83121956e+02\n",
            " 1.72921842e+02 1.98806211e+03 1.93922076e+02 4.40645010e+02]\n",
            "train accuracy: 0.204; test accuracy: 0.201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra - Applying KNN on IMDB\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(new_IMDB_X_train[:,:25000], IMDB_y_train)\n",
        "\n",
        "pred = knn.predict(new_IMDB_X_test)[0]\n",
        "\n",
        "print(\"Test Prediction for 0 : \", pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn6xQgIdCME",
        "outputId": "04f0c1fa-d736-406c-bcf2-d9a99014c2d4"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Prediction for 0 :  1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [KNeighborsClassifier(),\n",
        "          DecisionTreeClassifier(),\n",
        "          sk_LogisticRegression()]\n",
        "\n",
        "perf = {}\n",
        "\n",
        "# our implementation is slow you may try sklearn version\n",
        "logitreg = sk_LogisticRegression()\n",
        "fit = logitreg.fit(new_IMDB_X_train, IMDB_y_train)\n",
        "y_test_prob = fit.predict(new_IMDB_X_test)\n",
        "fpr, tpr, _ = roc_curve(IMDB_y_test, y_test_prob)\n",
        "auroc = roc_auc_score(IMDB_y_test, y_test_prob)\n",
        "perf[\"LogisticRegression (ours)\"] = {'fpr':fpr, 'tpr':tpr, 'auroc':auroc}\n",
        "\n",
        "for model in models:\n",
        "    fit = model.fit(new_IMDB_X_train, IMDB_y_train)\n",
        "    y_test_prob = fit.predict_proba(new_IMDB_X_test)[:,1]\n",
        "    fpr, tpr, _ = roc_curve(IMDB_y_test, y_test_prob)\n",
        "    auroc = roc_auc_score(IMDB_y_test, y_test_prob)\n",
        "    if type(model).__name__ == \"LogisticRegression\":\n",
        "        perf[\"LogisticRegression (sklearn)\"] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "    else:\n",
        "        perf[type(model).__name__] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "i = 0\n",
        "for model_name, model_perf in perf.items():\n",
        "    plt.plot(model_perf['fpr'], model_perf['tpr'],label=model_name)\n",
        "    plt.text(0.4, i+0.1, model_name + ': AUC = '+ str(round(model_perf['auroc'],2)))\n",
        "    i += 0.1\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title('ROC in predicting Titanic survivor')\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
        "plt.show()\n",
        "# plt.savefig(\"roc_curve.png\", bbox_inches='tight', dpi=300)\n",
        "# plt.close()"
      ],
      "metadata": {
        "id": "mdb0OA4VsNLU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "8e51811c-48e6-4572-c4c8-f4badbb659dd"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-918555e8d978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogitreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_IMDB_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMDB_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my_test_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_IMDB_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mperf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LogisticRegression (ours)\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'fpr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tpr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auroc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauroc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \"\"\"\n\u001b[1;32m    962\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2 : Multiclass Regression Class"
      ],
      "metadata": {
        "id": "7gEvPV8emgda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From class Multiclass Regression collab\n",
        "class Multinomial_logistic:\n",
        "    def __init__(self, nFeatures, nClasses):\n",
        "        self.W = np.random.rand(nFeatures, nClasses)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.exp(np.matmul(X, self.W))\n",
        "        return y_pred / y_pred.sum(axis=1).reshape(X.shape[0], 1)\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        return np.matmul(X.transpose(), self.predict(X) - y)\n",
        "\n",
        "    def ce(self, X, y):\n",
        "        return -np.sum(y * np.log(self.predict(X)))\n",
        "\n",
        "    # modify it to add stopping criteria (what can you think of?)\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None, lr=0.005, niter=100):\n",
        "        losses_train = np.zeros(niter)\n",
        "        losses_valid = np.zeros(niter)\n",
        "        for i in range(niter):\n",
        "            self.W = self.W - lr * self.grad(X, y)\n",
        "            loss_train = self.ce(X, y)\n",
        "            losses_train[i] = loss_train\n",
        "            if X_valid is not None and y_valid is not None:\n",
        "                loss_valid = self.ce(X_valid, y_valid)\n",
        "                losses_valid[i] = loss_valid\n",
        "                print(f\"iter {i}: {loss_train:.3f}; {loss_valid:.3f}\")\n",
        "            else:\n",
        "                print(f\"iter {i}: {loss_train:.3f}\")\n",
        "        return losses_train, losses_valid\n",
        "\n",
        "    def check_grad(self, X, y):\n",
        "        N, C = y.shape\n",
        "        D = X.shape[1]\n",
        "\n",
        "        diff = np.zeros((D, C))\n",
        "\n",
        "        W = self.W.copy()\n",
        "\n",
        "        for i in range(D):\n",
        "            for j in range(C):\n",
        "                epsilon = np.zeros((D, C))\n",
        "                epsilon[i, j] = np.random.rand() * 1e-4\n",
        "\n",
        "                self.W = self.W + epsilon\n",
        "                J1 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                self.W = self.W - epsilon\n",
        "                J2 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                numeric_grad = (J1 - J2) / (2 * epsilon[i, j])\n",
        "                derived_grad = self.grad(X, y)[i, j]\n",
        "\n",
        "                diff[i, j] = np.square(derived_grad - numeric_grad).sum() / \\\n",
        "                             np.square(derived_grad + numeric_grad).sum()\n",
        "\n",
        "        # print(diff)\n",
        "        return diff.sum()\n",
        "\n",
        "    def evaluate(self, y, y_pred):\n",
        "      accuracy = sum(y_pred.argmax(axis=1) == y.argmax(axis=1))\n",
        "      accuracy = accuracy / y.shape[0]\n",
        "\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "zol2Q2ZEmfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Multiclass Regression on the 4-class prediction from the 20-news-group data\n",
        "\n",
        "# Split the data into training, validation, and testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "    X, y, test_size = 0.33, random_state=1, shuffle=True)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
        "    X_train, y_train, test_size = 0.5, random_state=1, shuffle=True)\n",
        "\n",
        "#Create model object\n",
        "D = X.shape[1]\n",
        "C = y.shape[1]\n",
        "\n",
        "mlr = Multinomial_logistic(D, C)\n",
        "\n",
        "# check grad\n",
        "print(mlr.check_grad(X_train, y_train))\n",
        "\n"
      ],
      "metadata": {
        "id": "pNCIdH_Ws_a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.3 : Method comparison using ROC curve"
      ],
      "metadata": {
        "id": "Qgm175hZsNum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 : Running Experiments"
      ],
      "metadata": {
        "id": "ROWGHspagVyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 : Z-Scores\n",
        "Report top 10 features with most positive/negative z-scores on the IMDB data using simple linear regression on the movie rating scores."
      ],
      "metadata": {
        "id": "qYBoSlP8gZKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2.1: ROC and AUROC for Binary Classification"
      ],
      "metadata": {
        "id": "tmEUPQuRgl6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2.2 : Classification Accuracy for Multi-class Prediction"
      ],
      "metadata": {
        "id": "yN7vFE2EgvTG"
      }
    }
  ]
}
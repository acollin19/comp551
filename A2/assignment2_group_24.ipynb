{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPzkF5UdaKVQkE1iTBvFVeV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acollin19/comp551/blob/Angele/A2/assignment2_group_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Process:\n",
        "\n",
        "### Task 1: Data Processing  \n",
        "\n",
        "\n",
        "1. IMDB Reviews\n",
        "2. 20 news groups: a multi-class labelled textual dataset\n",
        "\n",
        "\n",
        "### Task 2: Implement Logistic and Multiclass classifiers\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Multiclass Regression\n",
        "3. Comparisons using ROC curve\n",
        "\n",
        "\n",
        "### Task 3: Running Experiments\n",
        "\n",
        "\n",
        "\n",
        "1.    Most positive/negative Z-scores\n",
        "2.   Implement:  \n",
        "    * Binary classification on the IMDB Reviews\n",
        "\n",
        "  *   Multi-class classification on the 20 news group dataset\n",
        "\n",
        "3. Binary classification AUROC on IMDB data\n",
        "  *   Logistic Regression \n",
        "  *   KNN\n",
        "\n",
        "4. Multiclass classification accuracy\n",
        "\n",
        "5. Compare Accuracy of Models (Plot)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MPLXGJNqqK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zk8gZ35HpnLe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from scipy.sparse import csr_matrix\n",
        "import scipy.sparse as sp\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Multiclass Regression and ROC curve comparisons\n",
        "from sklearn import model_selection\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression as sk_LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from nltk import word_tokenize, download\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!ls aclImdb\n",
        "!ls aclImdb/test\n",
        "!ls aclImdb/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHwlrYyNIEVK",
        "outputId": "a4200804-9aff-4495-a135-11cca5ae23ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  11.5M      0  0:00:06  0:00:06 --:--:-- 18.8M\n",
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n",
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB dataset preprocession\n",
        "IMDB_X_train, IMDB_y_train= load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "IMDB_X_test, IMDB_y_test = load_svmlight_file('aclImdb/test/labeledBow.feat') \n"
      ],
      "metadata": {
        "id": "RNMsl4tlfmJD"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 : Data processing\n",
        "\n"
      ],
      "metadata": {
        "id": "sQE1MfRpbM2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1 : IMDB\n",
        "\n",
        " - X contain features based on the words in the movie reviews\n",
        " - y contain labels for whether the review score"
      ],
      "metadata": {
        "id": "kMcXOF5bbfbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st element in tuple represents the file its in, 2nd element represents the index of word it represents in vocab, float represents frequency\n",
        "# want to remove the words that appear in less than 1% of all documents # only appear in 250 documents\n",
        "# want to remove the words that appear in more than 50% of all documents # appear in 12500 documents\n",
        "# sign = sum(IMDB_X_train.sign()) # 1 if the word appears in the text 0 if not\n",
        "# good = ((sign < 12500) - (sign < 250))\n",
        "# indices = sp.find(good)\n",
        "\n",
        "sign = IMDB_X_train.sign()\n",
        "num_file_occurences_train = np.asarray(sign.sum(axis=0))# number of files each word appears in\n",
        "#ndarray of the indices of the words excluding stop words and rare words\n",
        "indices = np.intersect1d((np.flatnonzero(12500 > num_file_occurences_train)), (np.flatnonzero(250 < num_file_occurences_train)))\n",
        "\n",
        "\n",
        "# Trim dataset to indices\n",
        "IMDB_X_train = IMDB_X_train[:,indices]\n",
        "IMDB_X_test = IMDB_X_test[:,indices]\n",
        "\n",
        "# IMDB_y_train = np.array([IMDB_y_train[i] for i in indices])\n",
        "# IMDB_y_test = np.array([IMDB_y_test[i] for i in indices])\n",
        "\n",
        "# IMDB_X_train = IMDB_X_train.todense()\n",
        "print(len(IMDB_y_train))\n",
        "print(IMDB_X_train.shape)\n",
        "print(indices)"
      ],
      "metadata": {
        "id": "CvVZpw-SLx6f",
        "outputId": "cd1186df-5de9-4d38-b11a-20a3d4ac8b5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "(25000, 1734)\n",
            "[  21   23   27 ... 1933 1935 1938]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Z-Score Function"
      ],
      "metadata": {
        "id": "3k6XRFDaHUqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.utils.extmath import sparse\n",
        "# # Z - Score\n",
        "# def zscore(X,y):\n",
        "\n",
        "#   N = X.shape[0]\n",
        "#   std = (y - y.mean()) / y.std()\n",
        "#   z = sp.csr_matrix.dot(sp.csr_matrix.transpose(X),std)/ np.sqrt(N)\n",
        "#   absZ = np.abs(z)\n",
        "#   return (absZ,z)\n",
        "\n",
        "# zscores, noAbsZ = zscore(IMDB_X_train,IMDB_y_train)\n",
        "# # highestzindex = np.argsort(zscores)[-200:][::-1] # top 200 highest z scores\n",
        "# # lowestzindex = np.argsort(zscores)[:200][::1] # worst 200 highest z scores\n",
        "# # zindex = np.concatenate((highestzindex, lowestzindex))\n",
        "\n",
        "# #choose by their absolute value\n",
        "# zindex = np.argsort(noAbsZ)[::-1][:300]\n",
        "# zindex_notAbs = np.argsort(noAbsZ)[::-1][:300]\n",
        "# #words in zindex\n",
        "# wordsZ = noAbsZ[np.argsort(noAbsZ)[::-1][:300]]\n",
        "\n",
        "\n",
        "# # get correct indexed feature set\n",
        "# new_IMDB_X_train = IMDB_X_train[:,zindex].toarray()\n",
        "# new_IMDB_X_test = IMDB_X_test[:,zindex].toarray()\n",
        "\n",
        "\n",
        "# data = open('aclImdb/imdb.vocab')\n",
        "# vocab = data.read().split('\\n')\n",
        "\n",
        "# # get correct indexed word list\n",
        "# word_list = [vocab[i]for i in indices]\n",
        "\n",
        "# word_list = [word_list[i] for i in zindex]\n",
        "\n",
        "# posindexesZ = np.argsort(zindex)[-10:][::-1] # highest-lowest\n",
        "# negindexesZ = np.argsort(zindex)[:10][::1] # lowest-highest\n",
        "\n",
        "# pos = [word_list[i] for i in posindexesZ]\n",
        "# neg = [word_list[i] for i in negindexesZ]\n",
        "\n",
        "\n",
        "# print((noAbsZ))\n",
        "# print(len(word_list))\n",
        "# print(pos)\n",
        "# print(noAbsZ[np.argsort(noAbsZ)[-10:][::-1]])\n",
        "# print(neg)\n",
        "# print(noAbsZ[np.argsort(noAbsZ)[:10][::1]])"
      ],
      "metadata": {
        "id": "jKj3Uv9UHUKI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test \n",
        "from sklearn.utils.extmath import sparse\n",
        "# Map in df \n",
        "data = open('aclImdb/imdb.vocab')\n",
        "vocab = data.read().split('\\n')\n",
        "\n",
        "# get correct indexed word list\n",
        "word_list = [vocab[i]for i in indices]\n",
        "\n",
        "# Z - Score\n",
        "def zscore(X,y):\n",
        "\n",
        "  N = X.shape[0]\n",
        "  std = (y - y.mean()) / y.std()\n",
        "  z = sp.csr_matrix.dot(sp.csr_matrix.transpose(X),std)/ np.sqrt(N)\n",
        "  absZ = np.abs(z)\n",
        "  return (absZ,z)\n",
        "\n",
        "zscores, noAbsZ = zscore(IMDB_X_train,IMDB_y_train)\n",
        "\n",
        "data = list(zip(zscores,noAbsZ,word_list))\n",
        "df = pd.DataFrame.from_records(data, columns =['zscore', 'nonAbsZscore', 'word'])\n",
        "df['indices'] = df.index\n",
        "\n",
        "# get between [100,1000] range\n",
        "df = df.loc[101:400:1]\n",
        "zindices = df['indices'].tolist()\n",
        "\n",
        "# reindex word list with new indices\n",
        "word_list = [word_list[i] for i in zindices]\n",
        "\n",
        "# index feature and label sets\n",
        "imdb_X_train = IMDB_X_train[:,zindices].toarray()\n",
        "imdb_X_test = IMDB_X_test[:,zindices].toarray()\n",
        "\n",
        "\n",
        "print(len(IMDB_y_train))\n",
        "print(IMDB_X_train.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6UNw4a8WHp4",
        "outputId": "5f331f3d-e67c-4282-95d6-697929859c92"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000\n",
            "(25000, 1734)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading 20 news group dataset\n",
        "categories=['rec.sport.hockey','sci.electronics','talk.politics.guns','sci.space']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, remove=(['headers', 'footers', 'quotes']), shuffle=True, random_state=42)\n",
        "\n",
        "'''\n",
        "Cleaning: \n",
        "- filter out rare words, stopwords, and words that are not relevant to any of the 4 class labels.\n",
        "- MI to select the top M âˆˆ [10, 100] feature words per class and take the union of all top feature words to train your multiclass model.\n",
        "\n",
        "'''\n",
        "\n",
        "download('punkt') #tokenizer, run once\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "\n",
        "# Tokenize, remove stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    # Restricts string to alphabetic characters only\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "# text and ground truth labels\n",
        "texts, y = twenty_train.data, twenty_train.target\n",
        "\n",
        "corpus = [preprocess(text) for text in texts]\n"
      ],
      "metadata": {
        "id": "avXRl9Pq1YdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cc2b154-21d8-41dd-e20c-d4df7241f545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : Twenty Train\n",
        "\n",
        "Text preprocessing, tokenizing and filtering of stopwords are all included in the CountVectorizer function\n"
      ],
      "metadata": {
        "id": "mpsPgK1HpYIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data) # builds a dict of features and transforms docs to feature vectors\n",
        "X_train_counts.shape\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ],
      "metadata": {
        "id": "aQpmgOaTorEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = twenty_train.data # Test content\n",
        "\n",
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data) # builds a dict of features and transforms docs to feature vectors\n",
        "X_train_counts.shape\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X = X_train_tfidf\n",
        "X.todense()\n",
        "twenty_train_arr = X.toarray()\n",
        "\n",
        "y_cat = twenty_train.target\n",
        "# convert array of class indices to one-hot encoded array\n",
        "y = np.zeros((y_cat.size, y_cat.max() + 1))\n",
        "y[np.arange(y_cat.size), y_cat] = 1\n",
        "\n",
        "#print(zscore(twenty_train_arr))"
      ],
      "metadata": {
        "id": "fNeX7J3j7XHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 : Implementing Classes"
      ],
      "metadata": {
        "id": "hBsr2kCsa4xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 : Logistic Regression Class"
      ],
      "metadata": {
        "id": "rUqXu5jLlL0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic function\n",
        "logistic = lambda z: 1./ (1 + np.exp(-z))       #logistic function\n",
        "z = np.linspace(-5,5,50)\n",
        "\n",
        "# From class Logistic Regression collab\n",
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients \n",
        "        self.max_iters = max_iters                    #maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        N,D = x.shape\n",
        "        yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "        grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "        return grad\n",
        "        \n",
        "    def fit(self, x, y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x,np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf \n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x, y)\n",
        "            self.w = self.w - self.learning_rate * g \n",
        "            t += 1\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x,np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x,self.w))            #predict output\n",
        "        return yh\n",
        " "
      ],
      "metadata": {
        "id": "yviSnmE3mQPo"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation using Logistic Regression\n"
      ],
      "metadata": {
        "id": "lsz4yb9KIdEr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Logistic Regression on the IMDB data\n",
        "\n",
        "model = LogisticRegression(max_iters=1e3,learning_rate=.1, epsilon=1e-3)\n",
        "fit = model.fit(imdb_X_train,IMDB_y_train)\n",
        "\n",
        "y_train_pred = fit.predict(imdb_X_train)\n",
        "y_test_pred = fit.predict(imdb_X_test)\n",
        "\n",
        "# Threshold probabilities\n",
        "y_train_pred = (y_train_pred > 0.5).astype(int)\n",
        "y_test_pred = (y_test_pred > 0.5).astype(int)\n",
        "\n",
        "# Accuracy = correctly classified / total classified\n",
        "acc_train = sum(y_train_pred==IMDB_y_train)/len(IMDB_y_train)\n",
        "acc_test = sum(y_test_pred==IMDB_y_test)/len(IMDB_y_test)\n",
        "\n",
        "print(f\"train accuracy: {acc_train:.3f}; test accuracy: {acc_test:.3f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "# from sklearn.metrics import confusion_matrix\n",
        "# confusion_matrix(IMDB_y_test,y_test_pred)\n",
        "\n",
        "# # Gradient\n",
        "# gradient = model.gradient(imdb_X_train, IMDB_y_train) \n",
        "# gradient"
      ],
      "metadata": {
        "id": "LBTCwz2DcDwQ",
        "outputId": "6e2e2231-6a96-4ab9-eec2-315d53c4adf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 223,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train accuracy: 0.204; test accuracy: 0.201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "# Cross Validation Scores\n",
        "clf_lr = sk_LogisticRegression()\n",
        "scores = cross_val_score(clf_lr, imdb_X_train, IMDB_y_train, cv=10, n_jobs=4)\n",
        "scores"
      ],
      "metadata": {
        "id": "5YCMsJD7OLYN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef8fb643-2afe-4ada-f3d2-82d450dd17a8"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.3528, 0.3616, 0.3492, 0.3596, 0.3292, 0.3484, 0.3536, 0.3784,\n",
              "       0.3572, 0.3704])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2 : Multiclass Regression Class"
      ],
      "metadata": {
        "id": "7gEvPV8emgda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From class Multiclass Regression collab\n",
        "class Multinomial_logistic:\n",
        "    def __init__(self, nFeatures, nClasses):\n",
        "        self.W = np.random.rand(nFeatures, nClasses)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.exp(np.matmul(X, self.W))\n",
        "        return y_pred / y_pred.sum(axis=1).reshape(X.shape[0], 1)\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        return np.matmul(X.transpose(), self.predict(X) - y)\n",
        "\n",
        "    def ce(self, X, y):\n",
        "        return -np.sum(y * np.log(self.predict(X)))\n",
        "\n",
        "    # modify it to add stopping criteria (what can you think of?)\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None, lr=0.005, niter=100):\n",
        "        losses_train = np.zeros(niter)\n",
        "        losses_valid = np.zeros(niter)\n",
        "        for i in range(niter):\n",
        "            self.W = self.W - lr * self.grad(X, y)\n",
        "            loss_train = self.ce(X, y)\n",
        "            losses_train[i] = loss_train\n",
        "            if X_valid is not None and y_valid is not None:\n",
        "                loss_valid = self.ce(X_valid, y_valid)\n",
        "                losses_valid[i] = loss_valid\n",
        "                print(f\"iter {i}: {loss_train:.3f}; {loss_valid:.3f}\")\n",
        "            else:\n",
        "                print(f\"iter {i}: {loss_train:.3f}\")\n",
        "        return losses_train, losses_valid\n",
        "\n",
        "    def check_grad(self, X, y):\n",
        "        N, C = y.shape\n",
        "        D = X.shape[1]\n",
        "\n",
        "        diff = np.zeros((D, C))\n",
        "\n",
        "        W = self.W.copy()\n",
        "\n",
        "        for i in range(D):\n",
        "            for j in range(C):\n",
        "                epsilon = np.zeros((D, C))\n",
        "                epsilon[i, j] = np.random.rand() * 1e-4\n",
        "\n",
        "                self.W = self.W + epsilon\n",
        "                J1 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                self.W = self.W - epsilon\n",
        "                J2 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                numeric_grad = (J1 - J2) / (2 * epsilon[i, j])\n",
        "                derived_grad = self.grad(X, y)[i, j]\n",
        "\n",
        "                diff[i, j] = np.square(derived_grad - numeric_grad).sum() / \\\n",
        "                             np.square(derived_grad + numeric_grad).sum()\n",
        "\n",
        "        # print(diff)\n",
        "        return diff.sum()\n",
        "\n",
        "    def evaluate(self, y, y_pred):\n",
        "      accuracy = sum(y_pred.argmax(axis=1) == y.argmax(axis=1))\n",
        "      accuracy = accuracy / y.shape[0]\n",
        "\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "zol2Q2ZEmfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation using Multiclass Regression"
      ],
      "metadata": {
        "id": "T4uSeUfEIQPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Multiclass Regression on the 4-class prediction from the 20-news-group data\n",
        "\n",
        "# Split the data into training, validation, and testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "    X, y, test_size = 0.33, random_state=1, shuffle=True)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
        "    X_train, y_train, test_size = 0.5, random_state=1, shuffle=True)\n",
        "\n",
        "#Create model object\n",
        "D = X.shape[1]\n",
        "C = y.shape[1]\n",
        "\n",
        "mlr = Multinomial_logistic(D, C)\n",
        "\n",
        "# check grad\n",
        "print(mlr.check_grad(X_train, y_train))\n",
        "\n"
      ],
      "metadata": {
        "id": "pNCIdH_Ws_a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.3 : Method comparison using ROC curve"
      ],
      "metadata": {
        "id": "Qgm175hZsNum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [KNeighborsClassifier(),\n",
        "          DecisionTreeClassifier(),\n",
        "          sk_LogisticRegression()]\n",
        "\n",
        "perf = {}\n",
        "\n",
        "# our implementation is slow you may try sklearn version\n",
        "logitreg = sk_LogisticRegression()\n",
        "fit = logitreg.fit(new_IMDB_X_train, IMDB_y_train)\n",
        "y_test_prob = fit.predict(new_IMDB_X_test)\n",
        "fpr, tpr, _ = roc_curve(IMDB_y_test, y_test_prob)\n",
        "auroc = roc_auc_score(IMDB_y_test, y_test_prob)\n",
        "perf[\"LogisticRegression (ours)\"] = {'fpr':fpr, 'tpr':tpr, 'auroc':auroc}\n",
        "\n",
        "for model in models:\n",
        "    fit = model.fit(new_IMDB_X_train, IMDB_y_train)\n",
        "    y_test_prob = fit.predict_proba(new_IMDB_X_test)[:,1]\n",
        "    fpr, tpr, _ = roc_curve(IMDB_y_test, y_test_prob)\n",
        "    auroc = roc_auc_score(IMDB_y_test, y_test_prob)\n",
        "    if type(model).__name__ == \"LogisticRegression\":\n",
        "        perf[\"LogisticRegression (sklearn)\"] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "    else:\n",
        "        perf[type(model).__name__] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "i = 0\n",
        "for model_name, model_perf in perf.items():\n",
        "    plt.plot(model_perf['fpr'], model_perf['tpr'],label=model_name)\n",
        "    plt.text(0.4, i+0.1, model_name + ': AUC = '+ str(round(model_perf['auroc'],2)))\n",
        "    i += 0.1\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title('ROC in predicting Titanic survivor')\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
        "plt.show()\n",
        "# plt.savefig(\"roc_curve.png\", bbox_inches='tight', dpi=300)\n",
        "# plt.close()"
      ],
      "metadata": {
        "id": "mdb0OA4VsNLU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "8e51811c-48e6-4572-c4c8-f4badbb659dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-918555e8d978>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogitreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_IMDB_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMDB_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0my_test_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_IMDB_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_y_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mperf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LogisticRegression (ours)\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'fpr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tpr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'auroc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mauroc\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    961\u001b[0m     \"\"\"\n\u001b[1;32m    962\u001b[0m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0;32m--> 963\u001b[0;31m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    729\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 : Running Experiments"
      ],
      "metadata": {
        "id": "ROWGHspagVyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 : Z-Scores\n",
        "Report top 10 features with most positive/negative z-scores on the IMDB data using simple linear regression on the movie rating scores.  \n",
        "\n",
        "QUESTION TO ANSWER: Do they make sense for calling a movie good and bad, respectively?\n",
        "\n",
        "\n",
        "ANSWER: The top features with the most positive z-scores do make sense as it includes words like 'family', 'beutifully','excellent','wonderful' which are all generally used with a positive connotation. \n",
        "Likewise, the top features with the most negative make sense with calling a movie bad as it includes words such as 'worst', 'terrible', 'awful', 'stupid' which are all associated with negative connotation and can imply that a movie is bad.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qYBoSlP8gZKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negwords = df.sort_values('nonAbsZscore').head(10)\n",
        "poswords = df.sort_values('nonAbsZscore').tail(10)\n",
        "\n",
        "print(poswords)\n",
        "print(negwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjVakIxdCRRm",
        "outputId": "a0b0626f-79e6-4aff-b9ed-9bc934f8d68b"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       zscore  nonAbsZscore         word  indices\n",
            "182  5.605140      5.605140       family      182\n",
            "177  5.977714      5.977714       always      177\n",
            "202  6.116688      6.116688  performance      202\n",
            "152  6.285921      6.285921        young      152\n",
            "147  6.292276      6.292276        world      147\n",
            "270  6.345468      6.345468    beautiful      270\n",
            "163  6.584429      6.584429         both      163\n",
            "123  6.690121      6.690121        years      123\n",
            "342  7.337024      7.337024    wonderful      342\n",
            "282  8.154902      8.154902    excellent      282\n",
            "        zscore  nonAbsZscore      word  indices\n",
            "213  15.873323    -15.873323     worst      213\n",
            "129  10.515839    -10.515839   nothing      129\n",
            "106  10.054247    -10.054247       why      106\n",
            "341   9.249509     -9.249509     awful      341\n",
            "400   8.797149     -8.797149     waste      400\n",
            "198   8.324481     -8.324481   minutes      198\n",
            "346   7.993304     -7.993304  terrible      346\n",
            "122   7.756602     -7.756602     thing      122\n",
            "336   7.634698     -7.634698    stupid      336\n",
            "317   7.155281     -7.155281    boring      317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Experiemnts: Linear Regression for predicting ratings in the IMDB data"
      ],
      "metadata": {
        "id": "tmEUPQuRgl6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "LinR = LinearRegression()\n",
        "  \n",
        "LinR.fit(imdb_X_train, IMDB_y_train)\n",
        "print(LinR.score(imdb_X_train, IMDB_y_test))\n",
        "\n",
        "true_w = -4\n",
        "plt.clf()\n",
        "plt.plot(imdb_X_train, IMDB_y_train, '.')\n",
        "w_closedform = model.w\n",
        "plt.plot(imdb_X_train, true_w*imdb_X_train, 'b-', label='true model')\n",
        "plt.plot(imdb_X_train, imdb_X_train*w_closedform, 'g--', label='closed-form')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(r'$y=xw$')\n",
        "plt.legend()\n",
        "# plt.savefig(\"linear_fit_closedform.png\",bbox_inches='tight', dpi=300)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "h3HJD4xiFGou",
        "outputId": "7b2bacfe-2a2e-4c91-bc32-bde3237d27a1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.30359190059142616\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-eba69f184efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mw_closedform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_w\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimdb_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdb_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb_X_train\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mw_closedform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'closed-form'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'$y=xw$'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25000,300) (301,) "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNpXlsBJUAnVsJ//NfvfD9u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acollin19/comp551/blob/Angele/A2/assignment2_group_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Process:\n",
        "\n",
        "### Task 1: Data Processing  \n",
        "\n",
        "\n",
        "1. IMDB Reviews\n",
        "2. 20 news groups: a multi-class labelled textual dataset\n",
        "\n",
        "\n",
        "### Task 2: Implement Logistic and Multiclass classifiers\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Multiclass Regression\n",
        "3. Comparisons using ROC curve\n",
        "\n",
        "\n",
        "### Task 3: Running Experiments\n",
        "\n",
        "\n",
        "\n",
        "1.    Most positive/negative Z-scores\n",
        "2.   Implement:  \n",
        "    * Binary classification on the IMDB Reviews\n",
        "\n",
        "  *   Multi-class classification on the 20 news group dataset\n",
        "\n",
        "3. Binary classification AUROC on IMDB data\n",
        "  *   Logistic Regression \n",
        "  *   KNN\n",
        "\n",
        "4. Multiclass classification accuracy\n",
        "\n",
        "5. Compare Accuracy of Models (Plot)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MPLXGJNqqK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zk8gZ35HpnLe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Multiclass Regression and ROC curve comparisons\n",
        "from sklearn import model_selection\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression as sk_LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from nltk import word_tokenize, download\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!ls aclImdb\n",
        "!ls aclImdb/test\n",
        "!ls aclImdb/train\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHwlrYyNIEVK",
        "outputId": "fef507e3-47bb-480a-a48d-b7e64fd56aa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  37.5M      0  0:00:02  0:00:02 --:--:-- 37.5M\n",
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n",
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB dataset preprocession\n",
        "IMDB_X_train, IMDB_y_train= load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "IMDB_X_test, IMDB_y_test = load_svmlight_file('aclImdb/test/labeledBow.feat') \n",
        "\n",
        "# IMDB_y_train[IMDB_y_train < 5] = -1.0\n",
        "# IMDB_y_train[IMDB_y_train >= 5] = 1.0\n",
        "\n",
        "# vectors = load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "# vocab = pd.read_csv('aclImdb/imdb.vocab', names=['Vocab_List'])\n",
        "\n",
        "'''\n",
        "# Needs cleaning \n",
        "- choose the top D âˆˆ [100, 1000] features by their absolute z-score  using simple regression\n",
        "''' \n",
        "print(IMDB_X_train)"
      ],
      "metadata": {
        "id": "RNMsl4tlfmJD",
        "outputId": "0426ad3b-c15d-41ff-8c14-599044f15f3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t9.0\n",
            "  (0, 1)\t1.0\n",
            "  (0, 2)\t4.0\n",
            "  (0, 3)\t4.0\n",
            "  (0, 4)\t6.0\n",
            "  (0, 5)\t4.0\n",
            "  (0, 6)\t2.0\n",
            "  (0, 7)\t2.0\n",
            "  (0, 8)\t4.0\n",
            "  (0, 10)\t4.0\n",
            "  (0, 12)\t2.0\n",
            "  (0, 26)\t1.0\n",
            "  (0, 27)\t1.0\n",
            "  (0, 28)\t1.0\n",
            "  (0, 29)\t2.0\n",
            "  (0, 32)\t1.0\n",
            "  (0, 41)\t1.0\n",
            "  (0, 45)\t1.0\n",
            "  (0, 47)\t1.0\n",
            "  (0, 50)\t1.0\n",
            "  (0, 54)\t2.0\n",
            "  (0, 57)\t1.0\n",
            "  (0, 59)\t1.0\n",
            "  (0, 63)\t2.0\n",
            "  (0, 64)\t1.0\n",
            "  :\t:\n",
            "  (24999, 420)\t1.0\n",
            "  (24999, 421)\t1.0\n",
            "  (24999, 426)\t1.0\n",
            "  (24999, 427)\t1.0\n",
            "  (24999, 583)\t1.0\n",
            "  (24999, 585)\t1.0\n",
            "  (24999, 642)\t1.0\n",
            "  (24999, 679)\t2.0\n",
            "  (24999, 680)\t1.0\n",
            "  (24999, 1065)\t1.0\n",
            "  (24999, 1093)\t1.0\n",
            "  (24999, 1224)\t1.0\n",
            "  (24999, 1407)\t1.0\n",
            "  (24999, 1773)\t1.0\n",
            "  (24999, 3947)\t1.0\n",
            "  (24999, 4292)\t1.0\n",
            "  (24999, 4569)\t1.0\n",
            "  (24999, 4949)\t1.0\n",
            "  (24999, 5072)\t1.0\n",
            "  (24999, 5792)\t1.0\n",
            "  (24999, 5947)\t1.0\n",
            "  (24999, 9702)\t1.0\n",
            "  (24999, 12190)\t1.0\n",
            "  (24999, 12803)\t1.0\n",
            "  (24999, 15612)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st element in tuple represents the file its in, 2nd element represents the index of word it represents in vocab, float represents frequency\n",
        "# want to remove the words that appear in less than 1% of all documents # only appear in 250 documents\n",
        "# want to remove the words that appear in more than 50% of all documents # appear in 12500 documents\n",
        "import scipy.sparse as sp\n",
        "sign = sum(IMDB_X_train.sign()) # 1 if the word appears in the text 0 if not\n",
        "good = (sign < 12500) - (sign < 250)\n",
        "indices = sp.find(good)\n",
        "\n",
        "IMDB_X_train = IMDB_X_train[:,indices[1]]\n",
        "IMDB_X_train.todense()\n",
        "# arr = IMDB_X_train.toarray()"
      ],
      "metadata": {
        "id": "CvVZpw-SLx6f",
        "outputId": "095089f6-3c7c-4352-ee57-75eeb02284a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py:291: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using < is inefficient, try using >= instead.\n",
            "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Z-Score Function"
      ],
      "metadata": {
        "id": "3k6XRFDaHUqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(IMDB_X_train.toarray())\n",
        "df.insert(loc=0, column=\"A\", value=IMDB_y_train)\n",
        "IMDB_X_train = sp.csr_matrix(df.values)\n",
        "IMDB_X_train.todense()\n",
        "arr = IMDB_X_train.toarray()\n",
        "\n",
        "# Z - Score\n",
        "mean = sum(arr)/ arr.shape[0]\n",
        "differences = [(value - mean)**2 ] for value in arr]\n",
        "sum_of_differences = sum(differences)\n",
        "standard_deviation = (sum_of_differences / (len(arr) - 1)) ** 0.5\n",
        "zscores = [(value - mean) / standard_deviation for value in arr]\n",
        "\n",
        "print(arr)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "jKj3Uv9UHUKI",
        "outputId": "68ba3e76-823c-439b-b9ee-c712bfbe8a46"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-393ef8bb0fc0>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    differences = [(value - mean)**2 ] for value in arr]\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading 20 news group dataset\n",
        "categories=['rec.sport.hockey','sci.electronics','talk.politics.guns','sci.space']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, remove=(['headers', 'footers', 'quotes']), shuffle=True, random_state=42)\n",
        "\n",
        "'''\n",
        "Cleaning: \n",
        "- filter out rare words, stopwords, and words that are not relevant to any of the 4 class labels.\n",
        "- MI to select the top M âˆˆ [10, 100] feature words per class and take the union of all top feature words to train your multiclass model.\n",
        "\n",
        "'''\n",
        "\n",
        "download('punkt') #tokenizer, run once\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "\n",
        "# Tokenize, remove stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    # Restricts string to alphabetic characters only\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "\n",
        "# text and ground truth labels\n",
        "texts, y = twenty_train.data, twenty_train.target\n",
        "\n",
        "corpus = [preprocess(text) for text in texts]\n"
      ],
      "metadata": {
        "id": "avXRl9Pq1YdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7ed584-57fa-4322-dd4b-d1ff0fb12651"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 : Data processing\n",
        "\n"
      ],
      "metadata": {
        "id": "sQE1MfRpbM2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : IMDB\n",
        "\n",
        " - X contain features based on the words in the movie reviews\n",
        " - y contain labels for whether the review sentiment is positive 1 or negative -1"
      ],
      "metadata": {
        "id": "kMcXOF5bbfbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB train df\n",
        " \n",
        "# IMDB_X_train, IMDB_y_train = load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "# IMDB_X_test, IMDB_y_test = load_svmlight_file('aclImdb/test/labeledBow.feat') \n",
        "\n",
        "# IMDB_y_train[IMDB_y_train < 5] = -1.0\n",
        "# IMDB_y_train[IMDB_y_train >= 5] = 1.0\n",
        "\n",
        "# QUESTION TO ANSWER: Do they make sense for calling a movie good and bad, respectively?\n"
      ],
      "metadata": {
        "id": "buYXwTcjbpWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : Twenty Train\n",
        "\n",
        "Text preprocessing, tokenizing and filtering of stopwords are all included in the CountVectorizer function\n"
      ],
      "metadata": {
        "id": "mpsPgK1HpYIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data) # builds a dict of features and transforms docs to feature vectors\n",
        "X_train_counts.shape\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ],
      "metadata": {
        "id": "aQpmgOaTorEJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = twenty_train.data # Test content\n",
        "\n",
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data) # builds a dict of features and transforms docs to feature vectors\n",
        "X_train_counts.shape\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X = X_train_tfidf\n",
        "\n",
        "y_cat = twenty_train.target\n",
        "# convert array of class indices to one-hot encoded array\n",
        "y = np.zeros((y_cat.size, y_cat.max() + 1))\n",
        "y[np.arange(y_cat.size), y_cat] = 1\n",
        "print(X)"
      ],
      "metadata": {
        "id": "fNeX7J3j7XHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64abc8c-cf8c-4d61-aebd-817c7fb15125"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 30948)\t0.14040989982008775\n",
            "  (0, 30906)\t0.08405138460178864\n",
            "  (0, 30883)\t0.09530400819467387\n",
            "  (0, 30295)\t0.10290468881809638\n",
            "  (0, 30251)\t0.05264064004143937\n",
            "  (0, 28527)\t0.10189915843591614\n",
            "  (0, 28304)\t0.09697317574876095\n",
            "  (0, 28141)\t0.08274417530859261\n",
            "  (0, 28131)\t0.025242498432943575\n",
            "  (0, 27975)\t0.07298610652089194\n",
            "  (0, 27906)\t0.05419820732883213\n",
            "  (0, 27860)\t0.04363340430813677\n",
            "  (0, 27841)\t0.05366514697429806\n",
            "  (0, 27834)\t0.15692196174342432\n",
            "  (0, 27829)\t0.08962777594902022\n",
            "  (0, 27822)\t0.09928182630437254\n",
            "  (0, 27555)\t0.12458624583194353\n",
            "  (0, 26949)\t0.09928182630437254\n",
            "  (0, 26386)\t0.14441450882235457\n",
            "  (0, 26327)\t0.1334455497312062\n",
            "  (0, 26326)\t0.15800080004504655\n",
            "  (0, 26046)\t0.04679870001486382\n",
            "  (0, 26027)\t0.15800080004504655\n",
            "  (0, 25583)\t0.059145254902454356\n",
            "  (0, 24869)\t0.14004069997244845\n",
            "  :\t:\n",
            "  (2329, 30433)\t0.06815319170856916\n",
            "  (2329, 27883)\t0.21301289255201286\n",
            "  (2329, 26088)\t0.16672798990930832\n",
            "  (2329, 22608)\t0.2353383104271898\n",
            "  (2329, 20613)\t0.04767724465566593\n",
            "  (2329, 19638)\t0.2842172889836986\n",
            "  (2329, 18956)\t0.1593675602945682\n",
            "  (2329, 17791)\t0.14324581002331788\n",
            "  (2329, 17758)\t0.2400464577440715\n",
            "  (2329, 16060)\t0.21089882114893543\n",
            "  (2329, 16047)\t0.055149253840551825\n",
            "  (2329, 16000)\t0.25977779970544423\n",
            "  (2329, 14958)\t0.2842172889836986\n",
            "  (2329, 14423)\t0.2714060387072915\n",
            "  (2329, 13636)\t0.10131665353481777\n",
            "  (2329, 12755)\t0.11323037169677708\n",
            "  (2329, 11094)\t0.13813562038019717\n",
            "  (2329, 10689)\t0.37746975632202256\n",
            "  (2329, 10013)\t0.18537436226660256\n",
            "  (2329, 7644)\t0.21317436743926568\n",
            "  (2329, 7249)\t0.1875787506518855\n",
            "  (2329, 7153)\t0.07697579013900624\n",
            "  (2329, 4948)\t0.11218970381438816\n",
            "  (2329, 4803)\t0.048241049399280415\n",
            "  (2329, 4457)\t0.269921104219143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 : Implementing Classes"
      ],
      "metadata": {
        "id": "hBsr2kCsa4xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 : Logistic Regression Class"
      ],
      "metadata": {
        "id": "rUqXu5jLlL0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic function\n",
        "z = np.linspace(-5,5,100)\n",
        "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
        "\n",
        "# From class Logistic Regression collab\n",
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients \n",
        "        self.max_iters = max_iters                    #maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        N,D = x.shape\n",
        "        yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "        grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "        return grad\n",
        "        \n",
        "    def fit(self, x, y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x,np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf \n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x, y)\n",
        "            self.w = self.w - self.learning_rate * g \n",
        "            t += 1\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x,np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x,self.w))            #predict output\n",
        "        return yh\n",
        "\n",
        "# LogisticRegression.gradient = gradient  "
      ],
      "metadata": {
        "id": "yviSnmE3mQPo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Logistic Regression on the IMDB data\n",
        "\n",
        "model = LogisticRegression(verbose=True, add_bias=False)\n",
        "fit = model.fit(IMDB_X_train, IMDB_y_train) \n",
        "prediction = fit.predict(IMDB_X_train)\n",
        "\n",
        "lin_coefficients= pd.DataFrame(fit.w[:(len(fit.w)-1)]).transpose()\n",
        "print(lin_coefficients.to_string(index=False))\n",
        "\n",
        "# plt.plot(X_train, y_train, '.', label='dataset')\n",
        "# plt.plot(X_train, prediction, 'g', alpha=.5, label='predictions')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel(r'$y$')\n",
        "# plt.legend()\n",
        "# # plt.show()\n",
        "# plt.savefig('IMDB_prediction.png', bbox_inches=\"tight\", dpi=300)"
      ],
      "metadata": {
        "id": "LBTCwz2DcDwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "36923ad6-9fb5-4ffa-b2df-dac4ea7990f7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f9393582fcf6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMDB_y_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# DOESNT WORK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-78956cd5c659>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'ndim'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Check Gradient using small perturbation, \n",
        "monitor the cross-entropy as a function of iteration, \n",
        "and report your finding on both datasets\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "fcXaQvDWhBQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra - Applying KNN on IMDB\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(IMDB_X_train[:,:89523], IMDB_y_train)\n",
        "\n",
        "pred = knn.predict(IMDB_X_test)[0]\n",
        "\n",
        "print(\"Test Prediction for 0 : \", pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn6xQgIdCME",
        "outputId": "82586806-7a0d-4c2c-fb2f-aba6c836def7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Prediction for 0 :  -1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2 : Multiclass Regression Class"
      ],
      "metadata": {
        "id": "7gEvPV8emgda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From class Multiclass Regression collab\n",
        "class Multinomial_logistic:\n",
        "    def __init__(self, nFeatures, nClasses):\n",
        "        self.W = np.random.rand(nFeatures, nClasses)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.exp(np.matmul(X, self.W))\n",
        "        return y_pred / y_pred.sum(axis=1).reshape(X.shape[0], 1)\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        return np.matmul(X.transpose(), self.predict(X) - y)\n",
        "\n",
        "    def ce(self, X, y):\n",
        "        return -np.sum(y * np.log(self.predict(X)))\n",
        "\n",
        "    # modify it to add stopping criteria (what can you think of?)\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None, lr=0.005, niter=100):\n",
        "        losses_train = np.zeros(niter)\n",
        "        losses_valid = np.zeros(niter)\n",
        "        for i in range(niter):\n",
        "            self.W = self.W - lr * self.grad(X, y)\n",
        "            loss_train = self.ce(X, y)\n",
        "            losses_train[i] = loss_train\n",
        "            if X_valid is not None and y_valid is not None:\n",
        "                loss_valid = self.ce(X_valid, y_valid)\n",
        "                losses_valid[i] = loss_valid\n",
        "                print(f\"iter {i}: {loss_train:.3f}; {loss_valid:.3f}\")\n",
        "            else:\n",
        "                print(f\"iter {i}: {loss_train:.3f}\")\n",
        "        return losses_train, losses_valid\n",
        "\n",
        "    def check_grad(self, X, y):\n",
        "        N, C = y.shape\n",
        "        D = X.shape[1]\n",
        "\n",
        "        diff = np.zeros((D, C))\n",
        "\n",
        "        W = self.W.copy()\n",
        "\n",
        "        for i in range(D):\n",
        "            for j in range(C):\n",
        "                epsilon = np.zeros((D, C))\n",
        "                epsilon[i, j] = np.random.rand() * 1e-4\n",
        "\n",
        "                self.W = self.W + epsilon\n",
        "                J1 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                self.W = self.W - epsilon\n",
        "                J2 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                numeric_grad = (J1 - J2) / (2 * epsilon[i, j])\n",
        "                derived_grad = self.grad(X, y)[i, j]\n",
        "\n",
        "                diff[i, j] = np.square(derived_grad - numeric_grad).sum() / \\\n",
        "                             np.square(derived_grad + numeric_grad).sum()\n",
        "\n",
        "        # print(diff)\n",
        "        return diff.sum()\n",
        "\n",
        "    def evaluate(self, y, y_pred):\n",
        "      accuracy = sum(y_pred.argmax(axis=1) == y.argmax(axis=1))\n",
        "      accuracy = accuracy / y.shape[0]\n",
        "\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "zol2Q2ZEmfNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Multiclass Regression on the 4-class prediction from the 20-news-group data\n",
        "\n",
        "# Split the data into training, validation, and testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "    X, y, test_size = 0.33, random_state=1, shuffle=True)\n",
        "\n",
        "X_train, X_valid, y_train, y_valid = model_selection.train_test_split(\n",
        "    X_train, y_train, test_size = 0.5, random_state=1, shuffle=True)\n"
      ],
      "metadata": {
        "id": "pNCIdH_Ws_a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.3 : Method comparison using ROC curve"
      ],
      "metadata": {
        "id": "Qgm175hZsNum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [KNeighborsClassifier(),\n",
        "          DecisionTreeClassifier(),\n",
        "          sk_LogisticRegression()]\n",
        "\n",
        "perf = {}\n",
        "\n",
        "# our implementation is slow you may try sklearn version\n",
        "logitreg = LogisticRegression(max_iters=1e3)\n",
        "fit = logitreg.fit(X_train, y_train)\n",
        "y_test_prob = fit.predict(X_test)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
        "auroc = roc_auc_score(y_test, y_test_prob)\n",
        "perf[\"LogisticRegression (ours)\"] = {'fpr':fpr, 'tpr':tpr, 'auroc':auroc}\n",
        "\n",
        "for model in models:\n",
        "    fit = model.fit(X_train, y_train)\n",
        "    y_test_prob = fit.predict_proba(X_test)[:,1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
        "    auroc = roc_auc_score(y_test, y_test_prob)\n",
        "    if type(model).__name__ == \"LogisticRegression\":\n",
        "        perf[\"LogisticRegression (sklearn)\"] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "    else:\n",
        "        perf[type(model).__name__] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "i = 0\n",
        "for model_name, model_perf in perf.items():\n",
        "    plt.plot(model_perf['fpr'], model_perf['tpr'],label=model_name)\n",
        "    plt.text(0.4, i+0.1, model_name + ': AUC = '+ str(round(model_perf['auroc'],2)))\n",
        "    i += 0.1\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title('ROC in predicting Titanic survivor')\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
        "# plt.show()\n",
        "plt.savefig(\"roc_curve.png\", bbox_inches='tight', dpi=300)\n",
        "# plt.close()"
      ],
      "metadata": {
        "id": "mdb0OA4VsNLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 : Running Experiments"
      ],
      "metadata": {
        "id": "ROWGHspagVyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 : Z-Scores\n",
        "Report top 10 features with most positive/negative z-scores on the IMDB data using simple linear regression on the movie rating scores."
      ],
      "metadata": {
        "id": "qYBoSlP8gZKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2.1: ROC and AUROC for Binary Classification"
      ],
      "metadata": {
        "id": "tmEUPQuRgl6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2.2 : Classification Accuracy for Multi-class Prediction"
      ],
      "metadata": {
        "id": "yN7vFE2EgvTG"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acollin19/comp551/blob/adam/assignment2_group_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "\n",
        "## Process:\n",
        "\n",
        "### Task 1: Data Processing  \n",
        "\n",
        "\n",
        "1. IMDB Reviews\n",
        "2. 20 news groups: a multi-class labelled textual dataset\n",
        "\n",
        "\n",
        "### Task 2: Implement Logistic and Multiclass classifiers\n",
        "\n",
        "1. Logistic Regression\n",
        "2. Multiclass Regression\n",
        "3. Comparisons using ROC curve\n",
        "\n",
        "\n",
        "### Task 3: Running Experiments\n",
        "\n",
        "\n",
        "\n",
        "1.    Most positive/negative Z-scores\n",
        "2.   Implement:  \n",
        "    * Binary classification on the IMDB Reviews\n",
        "\n",
        "  *   Multi-class classification on the 20 news group dataset\n",
        "\n",
        "3. Binary classification AUROC on IMDB data\n",
        "  *   Logistic Regression \n",
        "  *   KNN\n",
        "\n",
        "4. Multiclass classification accuracy\n",
        "\n",
        "5. Compare Accuracy of Models (Plot)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3MPLXGJNqqK6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zk8gZ35HpnLe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_svmlight_file\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "\n",
        "# Multiclass Regression and ROC curve comparisons\n",
        "from sklearn import model_selection\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression as sk_LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from nltk import word_tokenize, download\n",
        "from nltk.corpus import stopwords\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHwlrYyNIEVK",
        "outputId": "f5c49ce3-9d9a-4c3c-9860-642a762dfe38"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  41.1M      0  0:00:01  0:00:01 --:--:-- 41.1M\n",
            "imdbEr.txt  imdb.vocab\tREADME\ttest  train\n",
            "labeledBow.feat  neg  pos  urls_neg.txt  urls_pos.txt\n",
            "labeledBow.feat  pos\tunsupBow.feat  urls_pos.txt\n",
            "neg\t\t unsup\turls_neg.txt   urls_unsup.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB dataset preprocession\n",
        "IMDB_X_train, IMDB_y_train= load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "IMDB_X_test, IMDB_y_test = load_svmlight_file('aclImdb/test/labeledBow.feat') \n",
        "\n",
        "# IMDB_y_train[IMDB_y_train < 5] = -1.0\n",
        "# IMDB_y_train[IMDB_y_train >= 5] = 1.0\n",
        "\n",
        "# vectors = load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "\n",
        "'''\n",
        "# Needs cleaning \n",
        "- choose the top D ∈ [100, 1000] features by their absolute z-score  using simple regression\n",
        "''' \n",
        "print(IMDB_X_train)"
      ],
      "metadata": {
        "id": "RNMsl4tlfmJD",
        "outputId": "cb168ac0-cd8f-41d9-99ab-03ea07142a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 0)\t9.0\n",
            "  (0, 1)\t1.0\n",
            "  (0, 2)\t4.0\n",
            "  (0, 3)\t4.0\n",
            "  (0, 4)\t6.0\n",
            "  (0, 5)\t4.0\n",
            "  (0, 6)\t2.0\n",
            "  (0, 7)\t2.0\n",
            "  (0, 8)\t4.0\n",
            "  (0, 10)\t4.0\n",
            "  (0, 12)\t2.0\n",
            "  (0, 26)\t1.0\n",
            "  (0, 27)\t1.0\n",
            "  (0, 28)\t1.0\n",
            "  (0, 29)\t2.0\n",
            "  (0, 32)\t1.0\n",
            "  (0, 41)\t1.0\n",
            "  (0, 45)\t1.0\n",
            "  (0, 47)\t1.0\n",
            "  (0, 50)\t1.0\n",
            "  (0, 54)\t2.0\n",
            "  (0, 57)\t1.0\n",
            "  (0, 59)\t1.0\n",
            "  (0, 63)\t2.0\n",
            "  (0, 64)\t1.0\n",
            "  :\t:\n",
            "  (24999, 420)\t1.0\n",
            "  (24999, 421)\t1.0\n",
            "  (24999, 426)\t1.0\n",
            "  (24999, 427)\t1.0\n",
            "  (24999, 583)\t1.0\n",
            "  (24999, 585)\t1.0\n",
            "  (24999, 642)\t1.0\n",
            "  (24999, 679)\t2.0\n",
            "  (24999, 680)\t1.0\n",
            "  (24999, 1065)\t1.0\n",
            "  (24999, 1093)\t1.0\n",
            "  (24999, 1224)\t1.0\n",
            "  (24999, 1407)\t1.0\n",
            "  (24999, 1773)\t1.0\n",
            "  (24999, 3947)\t1.0\n",
            "  (24999, 4292)\t1.0\n",
            "  (24999, 4569)\t1.0\n",
            "  (24999, 4949)\t1.0\n",
            "  (24999, 5072)\t1.0\n",
            "  (24999, 5792)\t1.0\n",
            "  (24999, 5947)\t1.0\n",
            "  (24999, 9702)\t1.0\n",
            "  (24999, 12190)\t1.0\n",
            "  (24999, 12803)\t1.0\n",
            "  (24999, 15612)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1st element in tuple represents the file its in, 2nd element represents the index of word it represents in vocab, float represents frequency\n",
        "# want to remove the words that appear in less than 1% of all documents # only appear in 250 documents\n",
        "# want to remove the words that appear in more than 50% of all documents # appear in 12500 documents\n",
        "import scipy.sparse as sp\n",
        "sign = sum(IMDB_X_train.sign()) # 1 if the word appears in the text 0 if not\n",
        "good = (sign < 12500) - (sign < 250)\n",
        "indices = sp.find(good)\n",
        "\n",
        "IMDB_X_train = IMDB_X_train[:,indices[1]]\n",
        "IMDB_X_train.todense()\n",
        "print(IMDB_X_train)"
      ],
      "metadata": {
        "id": "CvVZpw-SLx6f",
        "outputId": "a13d886a-676e-4820-e3bb-dc2177a68d33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py:291: SparseEfficiencyWarning: Comparing a sparse matrix with a scalar greater than zero using < is inefficient, try using >= instead.\n",
            "  warn(bad_scalar_msg, SparseEfficiencyWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 2)\t1.0\n",
            "  (0, 5)\t1.0\n",
            "  (0, 14)\t1.0\n",
            "  (0, 18)\t1.0\n",
            "  (0, 20)\t1.0\n",
            "  (0, 23)\t1.0\n",
            "  (0, 27)\t2.0\n",
            "  (0, 30)\t1.0\n",
            "  (0, 32)\t1.0\n",
            "  (0, 36)\t2.0\n",
            "  (0, 37)\t1.0\n",
            "  (0, 39)\t1.0\n",
            "  (0, 41)\t2.0\n",
            "  (0, 43)\t1.0\n",
            "  (0, 45)\t1.0\n",
            "  (0, 51)\t1.0\n",
            "  (0, 73)\t1.0\n",
            "  (0, 79)\t1.0\n",
            "  (0, 89)\t1.0\n",
            "  (0, 95)\t1.0\n",
            "  (0, 98)\t1.0\n",
            "  (0, 109)\t1.0\n",
            "  (0, 113)\t1.0\n",
            "  (0, 115)\t1.0\n",
            "  (0, 123)\t1.0\n",
            "  :\t:\n",
            "  (24999, 176)\t1.0\n",
            "  (24999, 178)\t1.0\n",
            "  (24999, 206)\t1.0\n",
            "  (24999, 228)\t1.0\n",
            "  (24999, 240)\t1.0\n",
            "  (24999, 272)\t1.0\n",
            "  (24999, 277)\t1.0\n",
            "  (24999, 285)\t1.0\n",
            "  (24999, 310)\t1.0\n",
            "  (24999, 317)\t1.0\n",
            "  (24999, 320)\t1.0\n",
            "  (24999, 364)\t1.0\n",
            "  (24999, 393)\t1.0\n",
            "  (24999, 394)\t1.0\n",
            "  (24999, 399)\t1.0\n",
            "  (24999, 400)\t1.0\n",
            "  (24999, 556)\t1.0\n",
            "  (24999, 558)\t1.0\n",
            "  (24999, 615)\t1.0\n",
            "  (24999, 652)\t2.0\n",
            "  (24999, 653)\t1.0\n",
            "  (24999, 1038)\t1.0\n",
            "  (24999, 1066)\t1.0\n",
            "  (24999, 1195)\t1.0\n",
            "  (24999, 1369)\t1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Z-Score Function"
      ],
      "metadata": {
        "id": "3k6XRFDaHUqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.extmath import sparse\n",
        "# Z - Score\n",
        "def zscore(X,y):\n",
        "\n",
        "  N = X.shape[0]\n",
        "  std = (IMDB_y_train- y.mean()) / y.std()\n",
        "  z = np. abs(sp.csr_matrix.dot (sp.csr_matrix.transpose(X), std))\n",
        "\n",
        "  return z\n",
        "\n",
        "\n",
        "df = pd.DataFrame(IMDB_X_train.toarray())\n",
        "df.insert(loc=0, column=\"A\", value=IMDB_y_train)\n",
        "IMDB_X_train = sp.csr_matrix(df.values)\n",
        "IMDB_X_train.todense()\n",
        "IMDB_arr = IMDB_X_train.toarray()\n",
        "\n",
        "zscores = zscore(IMDB_X_train,IMDB_y_train)\n",
        "\n",
        "zindex = np.argsort(zscores)[::-1][:400]\n",
        "\n",
        "# trim feature set\n",
        "IMDB_X_train = IMDB_X_train[:,zindex]\n",
        "IMDB_X_test = IMDB_X_test[:,zindex]\n",
        "\n",
        "# trim word list\n",
        "vocab = pd.read_csv('aclImdb/imdb.vocab', names=['Words'])\n",
        "\n",
        "vocab = vocab['Words'].tolist()\n",
        "\n",
        "vocab = [vocab[i] for i in zindex]\n",
        "\n",
        "print(vocab)\n",
        "print(zscores)\n",
        "print(zscores.shape[0])#1746"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKj3Uv9UHUKI",
        "outputId": "f497b600-9c40-4484-dbec-138a8a39856c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'you', 'she', 'a', 'time', 'at', 'with', '!', 'in', 'was', 'interesting', 'for', 'one', 'its', 'and', 'as', \"don't\", 'that', 'but', 'this', 'even', 'really', 'how', 'great', 'what', 'these', 'made', 'from', 'who', 'film', 'seen', 'well', 'so', 'to', 'me', 'enjoy', 'we', 'lost', 'had', 'no', 'series', 'main', 'need', 'when', 'better', 'it', 'home', 'be', 'classic', 'about', 'read', 'big', 'human', 'man', 'sure', 'old', 'only', 'during', 'i', 'let', \"doesn't\", 'another', 'called', 'is', 'watch', 'life', 'long', 'or', 'story', 'he', 'think', 'any', 'pretty', 'over', \"i've\", 'into', 'their', 'friend', 'both', 'down', 'us', 'have', 'movies', 'recommend', 'some', 'up', 'sex', 'on', 'little', 'writer', 'his', 'absolutely', 'see', 'there', 'where', 'why', 'after', 'thought', 'totally', 'good', 'them', 'show', 'someone', 'fact', 'then', 'movie', 'making', 'like', 'not', 'budget', 'bit', 'days', 'could', 'part', 'other', 'makes', 'again', 'nice', 'our', 'looking', 'woman', 'king', 'michael', 'fun', 'by', 'of', 'here', 'kind', 'actually', 'tv', 'others', 'day', 'years', 'horrible', 'which', 'now', 'dull', 'last', 'year', 'comes', 'wonder', 'music', 'very', 'say', 'oscar', 'short', 'two', 'space', 'least', 'also', 'within', 'turns', 'through', 'beautiful', 'they', 'everyone', 'scenes', 'all', 'director', 'same', 'her', 'few', 'dr', 'out', 'black', 'death', 'book', 'way', 'look', 'start', 'actor', 'has', 'each', 'thing', 'extremely', 'laugh', 'between', 'minutes', \"wasn't\", \"can't\", 'based', 'though', 'got', 'beginning', 'being', 'room', 'next', 'simply', 'may', 'wife', 'richard', 'effects', 'since', 'directors', 'feel', 'worst', 'been', 'sometimes', 'la', 'dog', 'want', 'says', 'such', 'definitely', 'put', 'anything', 'written', 'problem', 'performance', 'must', 'attempts', 'episode', 'murder', 'times', 'your', 'would', 'boring', 'know', 'turned', 'running', 'chance', 'de', 'still', 'feeling', 'certain', 'together', 'alone', 'scene', 'drama', 'college', 'slasher', 'myself', 'fantastic', 'role', 'more', 'especially', 'performances', 'jokes', \"didn't\", 'enjoyable', 'go', 'felt', 'took', 'original', \"wouldn't\", 'showing', \"i'm\", 'jane', 'can', 'sound', 'modern', 'hands', 'half', 'hardly', 'worked', 'nothing', 'certainly', 'couple', 'full', 'evil', 'thank', \"haven't\", 'use', 'does', 'children', 'favorite', 'anyway', 'experience', 'killing', 'genre', 'acting', 'person', 'different', 'overall', 'than', 'personal', 'playing', 'sort', 'personally', 'every', 'involved', 'exactly', 'if', 'ask', 'going', 'taking', 'dead', 'roles', 'while', 'girls', 'effect', 'loved', 'rate', 'tell', 'house', 'attempt', 'body', 'remember', 'kill', 'escape', 'despite', 'hours', 'plot', 'fan', 'having', 'sexy', 'family', 'view', 'found', 'count', 'fine', 'mean', 'strange', 'scott', 'computer', 's', 'play', 'style', 'everything', 'hate', 'poor', 'town', 'terrific', 'strong', 'yes', 'considered', 'seriously', 'money', 'mind', 'apparently', 'solid', 'else', 'near', 'wonderful', 'cannot', 'too', 'starts', 'romance', 'goes', 'army', 'maybe', 'sense', 'dad', 'middle', 'parts', 'horror', 'career', 'feature', 'single', 'wants', 'keep', 'bunch', 'police', 'silly', 'amusing', 'before', 'child', 'looks', 'boy', 'spoilers', 'happens', 'writers', 'ten', 'charm', 'note', 'rented', 'watched', 'recommended', 'gives', 'okay', 'new', 'itself', 'waste', 'telling', 'thriller', 'aspect', 'team', 'direction', 'fight', 'realistic', 'line', 'small', 'save', 'points', 'plays', 'brought', 'pleasure', 'actress', 'baby', 'did', 'attention', 'throughout', 'hand', 'rest', 'visual', 'constantly', 'mr', 'john', 'write', 'slow']\n",
            "[8.66601970e+04 2.31270840e+03 4.81251525e+03 ... 3.55614585e+01\n",
            " 6.85840813e+00 6.90126402e+01]\n",
            "1745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1 : Data processing\n",
        "\n"
      ],
      "metadata": {
        "id": "sQE1MfRpbM2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : IMDB\n",
        "\n",
        " - X contain features based on the words in the movie reviews\n",
        " - y contain labels for whether the review sentiment is positive 1 or negative -1"
      ],
      "metadata": {
        "id": "kMcXOF5bbfbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMDB train df\n",
        " \n",
        "# IMDB_X_train, IMDB_y_train = load_svmlight_file('aclImdb/train/labeledBow.feat')\n",
        "# IMDB_X_test, IMDB_y_test = load_svmlight_file('aclImdb/test/labeledBow.feat') \n",
        "\n",
        "# IMDB_y_train[IMDB_y_train < 5] = -1.0\n",
        "# IMDB_y_train[IMDB_y_train >= 5] = 1.0\n",
        "\n",
        "# QUESTION TO ANSWER: Do they make sense for calling a movie good and bad, respectively?\n"
      ],
      "metadata": {
        "id": "buYXwTcjbpWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2 : Twenty Train\n",
        "\n",
        "Text preprocessing, tokenizing and filtering of stopwords are all included in the CountVectorizer function\n"
      ],
      "metadata": {
        "id": "mpsPgK1HpYIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading 20 news group dataset\n",
        "categories=['rec.sport.hockey','sci.electronics','talk.politics.guns','sci.space']\n",
        "twenty_train = fetch_20newsgroups(subset='train', categories=categories, remove=(['headers', 'footers', 'quotes']), shuffle=True, random_state=42)\n",
        "\n",
        "'''\n",
        "Cleaning: \n",
        "- filter out rare words, stopwords, and words that are not relevant to any of the 4 class labels.\n",
        "- MI to select the top M ∈ [10, 100] feature words per class and take the union of all top feature words to train your multiclass model.\n",
        "\n",
        "'''\n",
        "\n",
        "download('punkt') #tokenizer, run once\n",
        "download('stopwords') #stopwords dictionary, run once\n",
        "\n",
        "# Tokenize, remove stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "avXRl9Pq1YdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7644426-77c8-4f9f-9d94-9c273b7b27af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    doc = word_tokenize(text)\n",
        "    doc = [word for word in doc if word not in stop_words]\n",
        "    # Restricts string to alphabetic characters only\n",
        "    doc = [word for word in doc if word.isalpha()] \n",
        "    return doc\n",
        "\n",
        "\n",
        "# text and ground truth labels\n",
        "texts, y = twenty_train.data, twenty_train.target\n",
        "\n",
        "corpus = [' '.join(preprocess(text)) for text in texts]"
      ],
      "metadata": {
        "id": "X6Bs2UamPijd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the text content into feature vectors\n",
        "count_vect = CountVectorizer() \n",
        "X_train_counts = count_vect.fit_transform(corpus) # builds a dict of features and transforms docs to feature vectors\n",
        "\n",
        "# Occurences to frequences : TF_IDF\n",
        "\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "\n",
        "# X = X_train_tfidf\n",
        "# X.todense()\n",
        "X_train_all = X_train_tfidf.toarray()\n",
        "X_train_all.shape"
      ],
      "metadata": {
        "id": "aQpmgOaTorEJ",
        "outputId": "a3c71761-40b5-40b3-9744-3fc680ebe2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2330, 23593)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_cat = twenty_train.target\n",
        "# convert array of class indices to one-hot encoded array\n",
        "y_train = np.zeros((y_cat.size, y_cat.max() + 1))\n",
        "y_train[np.arange(y_cat.size), y_cat] = 1\n",
        "\n",
        "y_train.shape\n",
        "#print(zscore(twenty_train_arr))"
      ],
      "metadata": {
        "id": "gXdJZ1JyRg2m",
        "outputId": "66c79c3b-1fa1-486a-97f6-6b54f68558a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2330, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test data\n",
        "\n",
        "twenty_train_test = fetch_20newsgroups(subset='test', categories=categories, remove=(['headers', 'footers', 'quotes']), shuffle=True, random_state=42)\n",
        "\n",
        "X_new_counts = count_vect.transform(twenty_train_test.data)\n",
        "X_test_all = tfidf_transformer.transform(X_new_counts)\n",
        "X_test_all.shape"
      ],
      "metadata": {
        "id": "NXZ40IjITL8y",
        "outputId": "887e795f-5250-4b0e-91dd-4f507d758b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1550, 23593)"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_cat = twenty_train_test.target\n",
        "# convert array of class indices to one-hot encoded array\n",
        "y_test = np.zeros((y_cat.size, y_cat.max() + 1))\n",
        "y_test[np.arange(y_cat.size), y_cat] = 1\n",
        "y_test.shape"
      ],
      "metadata": {
        "id": "i121B9KzT2xi",
        "outputId": "2ece75c4-0cd9-4a27-be63-58bc65047e34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1550, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = y_train.astype(int)\n",
        "X_counts = X_train_counts.toarray()\n",
        "# [i for i in X_train[0] if i != 0]"
      ],
      "metadata": {
        "id": "Jj0GL0xiZQjX"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mutual information\n",
        "from sklearn.metrics import mutual_info_score\n",
        "\n",
        "# MI_score = [ [0]*(y_train.shape[1]) for i in range(X_counts.shape[1])]\n",
        "MI_score = np.zeros((X_counts.shape[1], y_train.shape[1]))\n",
        "\n",
        "# mutual_info_score(X_counts[:,0], y_train[:,0])\n",
        "\n",
        "for i in range(X_train_all.shape[1]):\n",
        "  for j in range(y_train.shape[1]):\n",
        "    MI_score[i][j] = mutual_info_score(X_counts[:,i], y_train[:,j])"
      ],
      "metadata": {
        "id": "xVup5406Umki"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_MI_indices = np.zeros(( y_train.shape[1], 100), dtype=int)\n",
        "for i in range(y_train.shape[1]):\n",
        "  best_MI_indices[i] = np.argpartition(MI_score[:,i],-100)[-100:]"
      ],
      "metadata": {
        "id": "xJP1uPSkUBAY"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_indices = np.unique(best_MI_indices.flatten())\n",
        "len(best_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6L__OhgOp4p",
        "outputId": "cd7facaf-f319-4a35-efcd-63766f06c921"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "329"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# count_vect.get_feature_names_out()[best_indices]"
      ],
      "metadata": {
        "id": "8q-QVFCrWg07"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X_train_all[:,best_indices]\n",
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeu5ohpZYa2u",
        "outputId": "7b0c09dd-9d7c-4aa6-e762-be8d25d714a6"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2330, 329)"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = X_test_all.toarray()[:,best_indices]\n",
        "X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLvnNG96ZYmp",
        "outputId": "2880265a-fcfe-4c34-9116-3dab153f75b5"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1550, 329)"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2 : Implementing Classes"
      ],
      "metadata": {
        "id": "hBsr2kCsa4xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 : Logistic Regression Class"
      ],
      "metadata": {
        "id": "rUqXu5jLlL0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic function\n",
        "z = np.linspace(-5,5,100)\n",
        "logistic = lambda z: 1./ (1 + np.exp(-z))\n",
        "\n",
        "# From class Logistic Regression collab\n",
        "class LogisticRegression:\n",
        "    \n",
        "    def __init__(self, add_bias=True, learning_rate=.1, epsilon=1e-4, max_iters=1e5, verbose=False):\n",
        "        self.add_bias = add_bias\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epsilon = epsilon                        #to get the tolerance for the norm of gradients \n",
        "        self.max_iters = max_iters                    #maximum number of iteration of gradient descent\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def gradient(self, x, y):\n",
        "        N,D = x.shape\n",
        "        yh = logistic(np.dot(x, self.w))    # predictions  size N\n",
        "        grad = np.dot(x.T, yh - y)/N        # divide by N because cost is mean over N points\n",
        "        return grad\n",
        "        \n",
        "    def fit(self, x, y):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        if self.add_bias:\n",
        "            N = x.shape[0]\n",
        "            x = np.column_stack([x,np.ones(N)])\n",
        "        N,D = x.shape\n",
        "        self.w = np.zeros(D)\n",
        "        g = np.inf \n",
        "        t = 0\n",
        "        # the code snippet below is for gradient descent\n",
        "        while np.linalg.norm(g) > self.epsilon and t < self.max_iters:\n",
        "            g = self.gradient(x, y)\n",
        "            self.w = self.w - self.learning_rate * g \n",
        "            t += 1\n",
        "        \n",
        "        if self.verbose:\n",
        "            print(f'terminated after {t} iterations, with norm of the gradient equal to {np.linalg.norm(g)}')\n",
        "            print(f'the weight found: {self.w}')\n",
        "        return self\n",
        "    \n",
        "    def predict(self, x):\n",
        "        if x.ndim == 1:\n",
        "            x = x[:, None]\n",
        "        Nt = x.shape[0]\n",
        "        if self.add_bias:\n",
        "            x = np.column_stack([x,np.ones(Nt)])\n",
        "        yh = logistic(np.dot(x,self.w))            #predict output\n",
        "        return yh\n",
        "\n",
        "# LogisticRegression.gradient = gradient  "
      ],
      "metadata": {
        "id": "yviSnmE3mQPo"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying Logistic Regression on the IMDB data\n",
        "\n",
        "model = LogisticRegression(verbose=True, add_bias=False)\n",
        "fit = model.fit(IMDB_X_train,IMDB_y_train).predict(IMDB_X_train)\n",
        "\n",
        "# plt.plot(X_train, y_train, '.', label='dataset')\n",
        "# plt.plot(X_train, prediction, 'g', alpha=.5, label='predictions')\n",
        "# plt.xlabel('x')\n",
        "# plt.ylabel(r'$y$')\n",
        "# plt.legend()\n",
        "# # plt.show()\n",
        "# plt.savefig('IMDB_prediction.png', bbox_inches=\"tight\", dpi=300)"
      ],
      "metadata": {
        "id": "LBTCwz2DcDwQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "825c8305-9c5e-4e1e-9f23-6039962a4314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: exp not found",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-e8534439f47a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0myh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_X_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMDB_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIMDB_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# lin_coefficients= pd.DataFrame(fit.w[:(len(fit.w)-1)]).transpose()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-78956cd5c659>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# the code snippet below is for gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-78956cd5c659>\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0myh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# predictions  size N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myh\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mN\u001b[0m        \u001b[0;31m# divide by N because cost is mean over N points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-78956cd5c659>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(z)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Logistic function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogistic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# From class Logistic Regression collab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: loop of ufunc does not support argument 0 of type csr_matrix which has no callable exp method"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Check Gradient using small perturbation, \n",
        "monitor the cross-entropy as a function of iteration, \n",
        "and report your finding on both datasets\n",
        "'''\n",
        "# Initialize the gradient method of the LogisticRegression class with gradient function\n",
        "# LogisticRegression.gradient = gradient             "
      ],
      "metadata": {
        "id": "fcXaQvDWhBQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extra - Applying KNN on IMDB\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(IMDB_X_train[:,:89523], IMDB_y_train)\n",
        "\n",
        "pred = knn.predict(IMDB_X_test)[0]\n",
        "\n",
        "print(\"Test Prediction for 0 : \", pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn6xQgIdCME",
        "outputId": "5a087b31-38bc-4740-ef41-a216f4287a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Prediction for 0 :  4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.2 : Multiclass Regression Class"
      ],
      "metadata": {
        "id": "7gEvPV8emgda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From class Multiclass Regression collab\n",
        "class Multinomial_logistic:\n",
        "    def __init__(self, nFeatures, nClasses):\n",
        "        self.W = np.random.rand(nFeatures, nClasses)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.exp(np.matmul(X, self.W))\n",
        "        return y_pred / y_pred.sum(axis=1).reshape(X.shape[0], 1)\n",
        "\n",
        "    def grad(self, X, y):\n",
        "        return np.matmul(X.transpose(), self.predict(X) - y)\n",
        "\n",
        "    def ce(self, X, y):\n",
        "        return -np.sum(y * np.log(self.predict(X)))\n",
        "\n",
        "    # modify it to add stopping criteria (what can you think of?)\n",
        "    def fit(self, X, y, X_valid=None, y_valid=None, lr=0.005, niter=100):\n",
        "        losses_train = np.zeros(niter)\n",
        "        losses_valid = np.zeros(niter)\n",
        "        for i in range(niter):\n",
        "            self.W = self.W - lr * self.grad(X, y)\n",
        "            loss_train = self.ce(X, y)\n",
        "            losses_train[i] = loss_train\n",
        "            if X_valid is not None and y_valid is not None:\n",
        "                loss_valid = self.ce(X_valid, y_valid)\n",
        "                losses_valid[i] = loss_valid\n",
        "                print(f\"iter {i}: {loss_train:.3f}; {loss_valid:.3f}\")\n",
        "            else:\n",
        "                print(f\"iter {i}: {loss_train:.3f}\")\n",
        "        return losses_train, losses_valid\n",
        "\n",
        "    def check_grad(self, X, y):\n",
        "        N, C = y.shape\n",
        "        D = X.shape[1]\n",
        "\n",
        "        diff = np.zeros((D, C))\n",
        "\n",
        "        W = self.W.copy()\n",
        "\n",
        "        for i in range(D):\n",
        "            for j in range(C):\n",
        "                epsilon = np.zeros((D, C))\n",
        "                epsilon[i, j] = np.random.rand() * 1e-4\n",
        "\n",
        "                self.W = self.W + epsilon\n",
        "                J1 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                self.W = self.W - epsilon\n",
        "                J2 = self.ce(X, y)\n",
        "                self.W = W\n",
        "\n",
        "                numeric_grad = (J1 - J2) / (2 * epsilon[i, j])\n",
        "                derived_grad = self.grad(X, y)[i, j]\n",
        "\n",
        "                diff[i, j] = np.square(derived_grad - numeric_grad).sum() / \\\n",
        "                             np.square(derived_grad + numeric_grad).sum()\n",
        "\n",
        "        # print(diff)\n",
        "        return diff.sum()\n",
        "\n",
        "    def evaluate(self, y, y_pred):\n",
        "      accuracy = sum(y_pred.argmax(axis=1) == y.argmax(axis=1))\n",
        "      accuracy = accuracy / y.shape[0]\n",
        "\n",
        "      return accuracy"
      ],
      "metadata": {
        "id": "zol2Q2ZEmfNI"
      },
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiclass for newsgroup\n",
        "\n",
        "mlr = Multinomial_logistic(X_train.shape[1], y_train.shape[1])\n",
        "mlr.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYiAsb9ijz6m",
        "outputId": "8465a0cd-8a56-45ab-9356-35e9eb7b09e6"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: 3221.194\n",
            "iter 1: 3190.343\n",
            "iter 2: 3160.098\n",
            "iter 3: 3130.457\n",
            "iter 4: 3101.419\n",
            "iter 5: 3072.982\n",
            "iter 6: 3045.141\n",
            "iter 7: 3017.892\n",
            "iter 8: 2991.229\n",
            "iter 9: 2965.144\n",
            "iter 10: 2939.632\n",
            "iter 11: 2914.683\n",
            "iter 12: 2890.288\n",
            "iter 13: 2866.438\n",
            "iter 14: 2843.123\n",
            "iter 15: 2820.332\n",
            "iter 16: 2798.055\n",
            "iter 17: 2776.280\n",
            "iter 18: 2754.997\n",
            "iter 19: 2734.192\n",
            "iter 20: 2713.856\n",
            "iter 21: 2693.976\n",
            "iter 22: 2674.541\n",
            "iter 23: 2655.539\n",
            "iter 24: 2636.959\n",
            "iter 25: 2618.789\n",
            "iter 26: 2601.019\n",
            "iter 27: 2583.636\n",
            "iter 28: 2566.631\n",
            "iter 29: 2549.992\n",
            "iter 30: 2533.710\n",
            "iter 31: 2517.773\n",
            "iter 32: 2502.173\n",
            "iter 33: 2486.900\n",
            "iter 34: 2471.943\n",
            "iter 35: 2457.295\n",
            "iter 36: 2442.945\n",
            "iter 37: 2428.886\n",
            "iter 38: 2415.109\n",
            "iter 39: 2401.606\n",
            "iter 40: 2388.370\n",
            "iter 41: 2375.391\n",
            "iter 42: 2362.664\n",
            "iter 43: 2350.180\n",
            "iter 44: 2337.934\n",
            "iter 45: 2325.918\n",
            "iter 46: 2314.126\n",
            "iter 47: 2302.552\n",
            "iter 48: 2291.189\n",
            "iter 49: 2280.033\n",
            "iter 50: 2269.076\n",
            "iter 51: 2258.314\n",
            "iter 52: 2247.742\n",
            "iter 53: 2237.354\n",
            "iter 54: 2227.145\n",
            "iter 55: 2217.111\n",
            "iter 56: 2207.248\n",
            "iter 57: 2197.550\n",
            "iter 58: 2188.013\n",
            "iter 59: 2178.634\n",
            "iter 60: 2169.408\n",
            "iter 61: 2160.331\n",
            "iter 62: 2151.400\n",
            "iter 63: 2142.610\n",
            "iter 64: 2133.959\n",
            "iter 65: 2125.444\n",
            "iter 66: 2117.060\n",
            "iter 67: 2108.804\n",
            "iter 68: 2100.674\n",
            "iter 69: 2092.666\n",
            "iter 70: 2084.778\n",
            "iter 71: 2077.007\n",
            "iter 72: 2069.350\n",
            "iter 73: 2061.804\n",
            "iter 74: 2054.367\n",
            "iter 75: 2047.037\n",
            "iter 76: 2039.810\n",
            "iter 77: 2032.685\n",
            "iter 78: 2025.660\n",
            "iter 79: 2018.732\n",
            "iter 80: 2011.898\n",
            "iter 81: 2005.158\n",
            "iter 82: 1998.509\n",
            "iter 83: 1991.949\n",
            "iter 84: 1985.476\n",
            "iter 85: 1979.089\n",
            "iter 86: 1972.784\n",
            "iter 87: 1966.562\n",
            "iter 88: 1960.420\n",
            "iter 89: 1954.356\n",
            "iter 90: 1948.370\n",
            "iter 91: 1942.458\n",
            "iter 92: 1936.621\n",
            "iter 93: 1930.855\n",
            "iter 94: 1925.161\n",
            "iter 95: 1919.536\n",
            "iter 96: 1913.980\n",
            "iter 97: 1908.490\n",
            "iter 98: 1903.066\n",
            "iter 99: 1897.706\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([3221.19416832, 3190.34325868, 3160.09772396, 3130.4569663 ,\n",
              "        3101.4192947 , 3072.98193837, 3045.14107398, 3017.89186595,\n",
              "        2991.22851844, 2965.14433735, 2939.63180031, 2914.68263234,\n",
              "        2890.28788533, 2866.43801907, 2843.12298219, 2820.33229146,\n",
              "        2798.05510826, 2776.28031121, 2754.99656438, 2734.19238057,\n",
              "        2713.85617947, 2693.97634055, 2674.54125071, 2655.53934688,\n",
              "        2636.95915367, 2618.78931639, 2601.01862963, 2583.63606182,\n",
              "        2566.63077593, 2549.99214673, 2533.70977491, 2517.7734983 ,\n",
              "        2502.1734005 , 2486.89981729, 2471.94334094, 2457.29482269,\n",
              "        2442.94537378, 2428.88636501, 2415.10942517, 2401.60643846,\n",
              "        2388.36954108, 2375.39111707, 2362.66379364, 2350.18043596,\n",
              "        2337.9341416 , 2325.91823469, 2314.12625985, 2302.55197597,\n",
              "        2291.18934995, 2280.03255028, 2269.07594074, 2258.31407406,\n",
              "        2247.74168565, 2237.35368744, 2227.14516179, 2217.11135556,\n",
              "        2207.24767429, 2197.54967651, 2188.01306823, 2178.63369762,\n",
              "        2169.40754973, 2160.33074155, 2151.39951707, 2142.61024259,\n",
              "        2133.9594022 , 2125.44359339, 2117.05952282, 2108.80400226,\n",
              "        2100.67394468, 2092.66636049, 2084.7783539 , 2077.00711948,\n",
              "        2069.34993877, 2061.8041771 , 2054.3672805 , 2047.03677274,\n",
              "        2039.81025247, 2032.68539051, 2025.65992723, 2018.73167001,\n",
              "        2011.89849086, 2005.15832409, 1998.50916409, 1991.94906316,\n",
              "        1985.47612954, 1979.08852537, 1972.78446484, 1966.56221239,\n",
              "        1960.42008091, 1954.35643016, 1948.36966509, 1942.45823433,\n",
              "        1936.62062873, 1930.8553799 , 1925.16105886, 1919.53627475,\n",
              "        1913.97967354, 1908.48993682, 1903.06578065, 1897.70595443]),\n",
              " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum(np.argmax(mlr.predict(X_test), axis=1) == np.argmax(y_test, axis=1)) / y_test.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eI_sKerkVIM",
        "outputId": "5d5324e1-da95-47aa-b733-ff76fc698718"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.787741935483871"
            ]
          },
          "metadata": {},
          "execution_count": 194
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.3 : Method comparison using ROC curve"
      ],
      "metadata": {
        "id": "Qgm175hZsNum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = [KNeighborsClassifier(),\n",
        "          DecisionTreeClassifier(),\n",
        "          sk_LogisticRegression()]\n",
        "\n",
        "perf = {}\n",
        "\n",
        "# our implementation is slow you may try sklearn version\n",
        "logitreg = LogisticRegression(max_iters=1e3)\n",
        "fit = logitreg.fit(X_train, y_train)\n",
        "y_test_prob = fit.predict(X_test)\n",
        "fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
        "auroc = roc_auc_score(y_test, y_test_prob)\n",
        "perf[\"LogisticRegression (ours)\"] = {'fpr':fpr, 'tpr':tpr, 'auroc':auroc}\n",
        "\n",
        "for model in models:\n",
        "    fit = model.fit(X_train, y_train)\n",
        "    y_test_prob = fit.predict_proba(X_test)[:,1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_test_prob)\n",
        "    auroc = roc_auc_score(y_test, y_test_prob)\n",
        "    if type(model).__name__ == \"LogisticRegression\":\n",
        "        perf[\"LogisticRegression (sklearn)\"] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "    else:\n",
        "        perf[type(model).__name__] = {'fpr':fpr,'tpr':tpr,'auroc':auroc}\n",
        "\n",
        "\n",
        "plt.clf()\n",
        "i = 0\n",
        "for model_name, model_perf in perf.items():\n",
        "    plt.plot(model_perf['fpr'], model_perf['tpr'],label=model_name)\n",
        "    plt.text(0.4, i+0.1, model_name + ': AUC = '+ str(round(model_perf['auroc'],2)))\n",
        "    i += 0.1\n",
        "\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title('ROC in predicting Titanic survivor')\n",
        "plt.legend(bbox_to_anchor=(1.04, 0.5), loc=\"upper left\")\n",
        "# plt.show()\n",
        "plt.savefig(\"roc_curve.png\", bbox_inches='tight', dpi=300)\n",
        "# plt.close()"
      ],
      "metadata": {
        "id": "mdb0OA4VsNLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 : Running Experiments"
      ],
      "metadata": {
        "id": "ROWGHspagVyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 : Z-Scores\n",
        "Report top 10 features with most positive/negative z-scores on the IMDB data using simple linear regression on the movie rating scores."
      ],
      "metadata": {
        "id": "qYBoSlP8gZKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2.1: ROC and AUROC for Binary Classification"
      ],
      "metadata": {
        "id": "tmEUPQuRgl6Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2.2 : Classification Accuracy for Multi-class Prediction"
      ],
      "metadata": {
        "id": "yN7vFE2EgvTG"
      }
    }
  ]
}